{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 36-705 Intermediate Statistics.\n",
    "\n",
    "### Practice Test II.\n",
    "\n",
    "### INSERT DATE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Let $X_1, X_2,..., X_n \\sim \\text{Uniform}(-\\theta, \\theta)$ where $\\theta > 0$.\n",
    "\n",
    "a) Find the maximum likelihood estimator $\\widehat{\\theta}_n$.\n",
    "\n",
    "b) Find a minimal sufficient statistic.\n",
    "\n",
    "c) Show that $\\widehat{\\theta}_n \\overset{p}{\\rightarrow} \\theta$.\n",
    "\n",
    "d) Find the limiting distribution of $n (\\theta - \\widehat{\\theta_n})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a)\n",
    "\n",
    "The PDF of $X_i \\sim \\text{Uniform}(-\\theta, \\theta)$ is:\n",
    "\n",
    "$$f_{X_i}(x_i; \\theta) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{2\\theta} \\quad &\\text{if} \\space -\\theta \\leq x_i \\leq \\theta \\\\\n",
    "0 \\quad & \\text{otherwise} \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "As the $X_i$ are IID, the likelihood function is the product of individual PDFs. This is evaluated for a specific realisation of the data, i.e. fixing $X_1 = x_1,..., X_n = x_n$, and varies as a function of $\\theta$ over a parameter space $\\Theta$:\n",
    "\n",
    "$$L(\\theta) = \\prod^n_{i=1} f_{X_i}(x_i; \\theta)$$\n",
    "\n",
    "The likelihood function $L(\\theta)$ will be 0 when any of the PDFs $f_{X_i}(x_i; \\theta)$ are 0. This occurs when either $-\\theta > \\min \\{X_1, ..., X_n \\}$ or when $\\theta < \\max \\{X_1, ... , X_n \\}$.\n",
    "\n",
    "At these values, there will be data points $X_k = x_k$ where $x_k < -\\theta$ or $x_k > \\theta$ which have a zero-probability of occurring, according to that particular parametrisation. The likelihood will then be 0 because given the observed values $X_1 = x_1, ... , X_n = x_n$, it will be highly implausible for that parametrisation to have generated the data.\n",
    "\n",
    "Hence the likelihood function is:\n",
    "\n",
    "$$L(\\theta) =\n",
    "\\begin{cases}\n",
    "0 \\quad &\\text{if} \\space -\\theta > X_{(1)} \\\\\n",
    "0 \\quad &\\text{if} \\space  \\theta < X_{(n)} \\\\\n",
    "\\left(\\frac{1}{2\\theta} \\right)^n \\quad &\\text{if} \\space \\theta > X_{(n)}, \\space -\\theta > X_{(1)} \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Which can be written as:\n",
    "\n",
    "$$L(\\theta) = \\left(\\frac{1}{2 \\theta} \\right)^n \\mathbb{I}(- \\theta \\leq X_{(1)}) \\mathbb{I}(X_{(n)} \\geq X_{(n)})$$\n",
    "\n",
    "In order to maximise this, we need to set the parameter $\\theta$ so that the constraints imposed by the indicator functions are met, otherwise the likelihood function becomes 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Let $X \\sim \\text{Bernoulli}(\\theta)$ be a single coin flip.\n",
    "\n",
    "Suppose that $\\theta \\in \\Theta = \\{1/3, 2/3\\}$. Hence $\\theta$ can only take two possible values.\n",
    "\n",
    "a) Find the maximum likelihood estimator.\n",
    "\n",
    "b) Let the loss function be:\n",
    "\n",
    "$$L(\\theta, \\widehat{\\theta}) = \n",
    "\\begin{cases}\n",
    "1 \\quad &\\text{if} \\space \\theta \\neq \\widehat{\\theta} \\\\\n",
    "0 \\quad &\\text{if} \\space \\theta = \\widehat{\\theta} \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Find the risk function of the maximum likelihood estimator. Since $\\theta$ only takes the values $1/3$ and $2/3$, you only need to find $R(1/3, \\widehat{\\theta})$ and $R(2/3, \\widehat{\\theta})$.\n",
    "\n",
    "c) Show that the maximum likelihood estimator is minimax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a)\n",
    "\n",
    "The maximum likelihood estimator in this instance will be $\\widehat{\\theta} = \\widehat{\\theta}(X) =  X$, that is, our observation of the single coin flip.\n",
    "\n",
    "The log likelihood function is:\n",
    "\n",
    "$$l(\\theta) = x\\log \\theta - (1-x)\\log (1- \\theta)$$\n",
    "\n",
    "Maximising this by computing derivatives, setting to 0 and solving for $\\theta$, we have that:\n",
    "\n",
    "$$l'(\\theta) = \\frac{x}{\\theta} - \\frac{1 - x}{(1 - \\theta)} = 0 \\implies (1 - \\theta) x - \\theta (1 - x) = 0 \\implies \\widehat{\\theta} = x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b)\n",
    "\n",
    "The loss function specified is known as zero-one loss. The risk function $R(\\theta, \\widehat{\\theta})$ is the average loss incurred by the estimator $\\widehat{\\theta}$ over all possible values that the data $X$ can take:\n",
    "\n",
    "$$\\begin{align}\n",
    "R(\\theta, \\widehat{\\theta}(X)) &= \\mathbb{E}_{\\theta}[L(\\theta, \\widehat{\\theta}(X)] \\\\\n",
    "&= \\sum_x L(\\theta, \\widehat{\\theta}(x)) f_X(x; \\theta) \\\\\n",
    "&= L(\\theta, \\widehat{\\theta}(0)) \\cdot P(X = 0) + L(\\theta, \\widehat{\\theta}(1)) \\cdot P(X = 1) \\\\\n",
    "&= (1 - \\theta) \\cdot \\mathbb{I}(\\theta \\neq 0) + \\theta \\cdot \\mathbb{I}(\\theta \\neq 1) \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Which is the risk function of the maximum likelihood estimator. We now have that:\n",
    "\n",
    "$$R(1/3, \\widehat{\\theta}) = \\frac{2}{3} \\mathbb{I}(1/3 \\neq 0) + \\frac{1}{3} \\mathbb{I}(1/3 \\neq 1) = 1$$\n",
    "\n",
    "And that:\n",
    "\n",
    "$$R(2/3, \\widehat{\\theta}) = \\frac{1}{3} \\mathbb{I}(2/3 \\neq 0) + \\frac{2}{3} \\mathbb{I}(2/3 \\neq 1) = 1$$\n",
    "\n",
    "Hence over the restricted parameter space $\\Theta = \\{ 1/3, 2/3 \\}$, the risk function $R(\\cdot, \\widehat{\\theta})$ for the maximum likelihood estimator $\\widehat{\\theta}$ is constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2c)\n",
    "\n",
    "\n",
    "To show that the maximum likelihood estimator $\\widehat{\\theta}$ is minimax, we use the result that Bayes estimators with respect to a prior $\\pi$ that have a constant risk function are minimax estimators. We now assume that the parameter of interest $\\theta$ is a random variable.\n",
    "\n",
    "Under zero-one loss, the Bayes estimator is the posterior mode.\n",
    "\n",
    "Noting that the likelihood function is a Bernoulli distribution, we use its conjugate prior, the Beta distribution, $\\pi(\\theta) = \\text{Beta}(\\alpha, \\beta)$ for hyperparameters $\\alpha, \\beta$. Omitting the normalisation constants, we have that:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(\\theta | X) &\\propto p(X | \\theta) \\pi(\\theta) \\\\\n",
    "&\\propto \\theta^X (1 - \\theta)^{1-X} \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1} \\\\\n",
    "&\\propto \\theta^{X + \\alpha - 1} (1 - \\theta)^{\\beta - X} \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Hence the posterior distribution is $\\text{Beta}(X + \\alpha, \\beta - X + 1)$. As the mode of a $\\text{Beta}(\\alpha, \\beta)$ distribution is $(\\alpha - 1) / (\\alpha + \\beta + 2)$, we have that the posterior mode, and hence Bayes estimator under a general $\\text{Beta}(\\alpha, \\beta)$ prior is:\n",
    "\n",
    "$$\\frac{X + \\alpha - 1}{(X + \\alpha) + (\\beta - X + 1) - 2} = \\frac{X + \\alpha - 1}{\\alpha + \\beta - 1}$$\n",
    "\n",
    "Hence the maximum likelihood estimator $\\widehat{\\theta}(X) = X$ is the posterior mode under a $\\text{Beta}(1,1)$ prior, which is also a uniform prior $\\pi(\\theta) = 1$.\n",
    "\n",
    "And so for zero-one loss, $\\widehat{\\theta}(X) = X$ is the Bayes estimator for a uniform prior $\\pi(\\theta) = 1$. As the risk function $R(\\cdot, \\widehat{\\theta})$ is constant, $\\widehat{\\theta}$ is minimax.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Let $X_1,..., X_n$ be IID with distribution $\\text{Binomial}(k, \\theta)$.\n",
    "\n",
    "a) Find a minimal sufficient statistic $S$ for $\\theta$.\n",
    "\n",
    "b) For each of the following say whether it is sufficeint, minimal sufficient, or not sufficient:\n",
    "\n",
    "$$T = X_1, \\quad T = \\sum_i X_i, \\quad T = \\left(X_1, \\sum_i X_i \\right), \\quad T = \\left(X_1, \\sum^n_{i=1} X_i \\right)$$\n",
    "\n",
    "c) Let $\\tau(\\theta) = P(X = 1) = k\\theta(1- \\theta)^{k-1}$. Define $U = 1$ if $X_1 = 1$ and $0$ otherwise. Show that $U$ is an unbiased estimator of $\\tau$.\n",
    "\n",
    "d) Find the maximum likelihood estimator $\\widehat{\\tau}$ of $\\tau$.\n",
    "\n",
    "e) Show that $\\widehat{\\tau} \\overset{p}{\\rightarrow} \\tau$.\n",
    "\n",
    "f) Find the limiting distribution of $\\widehat{\\tau}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3a)\n",
    "\n",
    "The parameter of interest is $\\theta$, so assume that the number of trials $k$ is fixed and known.\n",
    "\n",
    "The Binomial PMF is:\n",
    "\n",
    "$$f_{X_i}(x_i; \\theta) = P(X_i  = x_i) = \\binom{k}{x_i} \\theta^{x_i}(1- \\theta)^{k-x_i}$$\n",
    "\n",
    "Given two datasets $x^n = X_1, ... X_n$ and $y^n = Y_1, ... , Y_n$, we compute:\n",
    "\n",
    "$$R(x^n, y^n ; \\theta) = \\frac{P(x_n ; \\theta)}{P(y^n ; \\theta)}$$\n",
    "\n",
    "Which involves computing the following joint PMF, or probability as the $X_i$ are discrete:\n",
    "\n",
    "$$\\begin{align}\n",
    "f_{X^n}(x^n ; \\theta) &= \\prod^n_{i=1} \\binom{k}{x_i} \\theta^{x_i} (1 - \\theta)^{k - x_i} \\\\\n",
    "&= \\frac{(k!)^n}{\\prod^n_{i=1} x_i! (k- x_i)!} \\theta^{\\sum^n_{i=1} x_i} (1 - \\theta)^{nk - \\sum^n_{i=1} x_i} \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Hence we have that:\n",
    "\n",
    "$$R(x^n, y^n ; \\theta) = \\frac{\\frac{(k!)^n}{\\prod^n_{i=1} x_i! (k- x_i)!} \\theta^{\\sum^n_{i=1} x_i} (1 - \\theta)^{nk - \\sum^n_{i=1} x_i}}{\\frac{(k!)^n}{\\prod^n_{i=1} y_i! (k- y_i)!} \\theta^{\\sum^n_{i=1} y_i} (1 - \\theta)^{nk - \\sum^n_{i=1} y_i}}$$\n",
    "\n",
    "Now if $R$ does not depend on $\\theta$, then it must be the case that $\\sum^n_{i=1} x_i = \\sum^n_{i=1} y_i$. The reverse implication also holds true, and $R$ will be some constant $c$ that does not depend on $\\theta$.\n",
    "\n",
    "Hence $T(X^n) = \\sum^n_{i=1} X_i $ is a minimal sufficient statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3b)\n",
    "\n",
    "Given that we have found a minimal sufficient statistic (MSS) to be $T(X^n) = \\sum^n_{i=1} X_i $, we can use the definition that the MSS must be expressible as a function of another sufficient statistic $U$, that is it must be the case that $T = g(U)$ for some function $g(\\cdot)$. \n",
    "\n",
    "We can use this to test whether another statistic $U$ is sufficient. If the MSS cannot be expressed as a function of $U$, then $U$ is not sufficient. If the MSS can be expressed as a function of $U$ then $U$ is sufficient. If it is the case that the MSS can be expressed as a function of $U$, where the function $g(\\cdot)$ is 1-to-1, then $T$ and $U$ are equivalent, and $U$ is also an MSS.\n",
    "\n",
    "Hence:\n",
    "\n",
    "$T = X_1$ is not sufficient because no function $g(\\cdot)$ can be applied to it to get $S = \\sum^n_{i=1} X_i$.\n",
    "\n",
    "$T = \\sum_i X_i$ is minimal sufficient because it is exactly equivalent to the MSS.\n",
    "\n",
    "$\\mathbf{T} = \\left(X_1, \\sum_i X_i \\right)$ is sufficient because the MSS can be expressed as $S = \\sum^n_{i=1} X_i = g(\\mathbf{T})$ where:\n",
    "\n",
    "$$g(T) = \n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{pmatrix} ^T\n",
    "\\begin{pmatrix}\n",
    "X_1 \\\\\n",
    "\\sum_i X_i\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "However, it is not minimal sufficient, because we can replace $X_1$ in the 1st entry of $\\mathbf{T}$ with any other $X_i$ to still yield the same MSS, $S = \\sum^n_{i=1}$. Therefore the function $g(\\cdot)$ is not one-to-one, and therefore $\\mathbf{T}$ is not minimal sufficient, as it contains redundant information.\n",
    "\n",
    "$\\mathbf{T} = \\left(X_1,  \\sum^n_{i=2} \\right) X_i$ is minimal sufficient, as we note that the MSS can be expressed as $S = \\sum^n_{i=1} X_i = h(\\mathbf{T})$, where:\n",
    "\n",
    "$$h(\\mathbf{T}) =\n",
    "\\begin{pmatrix}\n",
    "X_1, \\sum^n_{i=2} X_i\n",
    "\\end{pmatrix}^T\n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "1\\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "which is one-to-one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3c)\n",
    "\n",
    "An estimator $\\widetilde{\\theta}$ of a parameter $\\theta$ is unbiased if we have that:\n",
    "\n",
    "$$\\mathbb{E}_{\\theta}[\\widetilde{\\theta}] = \\theta$$\n",
    "\n",
    "Where the expectation is with respect to the *** \n",
    "\n",
    "Noting that $U$ is a Bernoulli random variable with parameter $\\tau(\\theta)$, we have that\n",
    "\n",
    "$$\\mathbb{E}_{\\tau(\\theta)}[U] = \\mathbb{E}_{\\tau(\\theta)} [\\mathbb{I}(X_1 = 1)] = 1 \\cdot \\tau(\\theta) + 0 \\cdot (1 - \\tau(\\theta)) = \\tau(\\theta)$$\n",
    "\n",
    "Hence $U$ is an unbiased estimator of $\\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3d)\n",
    "\n",
    "We first compute the maximum likelihood estimator $\\widehat{\\theta}$ of $\\theta$. The log-likelihood is:\n",
    "\n",
    "$$l(\\theta) = \\log f_{X^n}(x^n; \\theta) = \\sum^n_{i=1} \\log \\binom{k}{x_i} + \\left(\\sum^n_{i=1} x_i \\right) \\log \\theta + \\left(nk - \\sum^n_{i=1} x_i \\right) \\log(1 - \\theta)$$\n",
    "\n",
    "Taking derivatives, setting to 0, and solving for the parameter to maximise the log-likelihood, we have\n",
    "\n",
    "$$\\begin{align}\n",
    "l'(\\theta) &= \\frac{\\sum^n_{i=1} x_i}{\\theta} - \\frac{nk - \\sum^n_{i=1} x_i}{1-\\theta} \\\\\n",
    "&= \\frac{(1- \\theta) \\sum^n_{i=1} x_i - \\theta \\left( nk - \\sum^n_{i=1} x_i \\right)}{\\theta(1- \\theta)} \\\\\n",
    "&= \\sum^n_{i=1} x_i - \\theta \\sum^n_{i=1} x_i - \\theta nk + \\theta \\sum^n_{i=1} x_i = 0 \\\\\n",
    "\\implies \\widehat{\\theta} &= \\frac{\\sum^n_{i=1} x_i}{nk} \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Hence the maximum likelihood estimator is $\\widehat{\\theta} = k^{-1} \\bar{X}_n$ which is the proportion of success events out of the maximum number of successes observable in $n$ experiments, where each experiment consists of $k$ trials.\n",
    "\n",
    "Noting that $\\tau$ is a function of the parameter $\\theta$, we can use the equivariance of the MLE to get the maximum likelihood estimator $\\widehat{\\tau}$ of $\\tau$, using our estimate $\\widehat{\\theta}$:\n",
    "\n",
    "$$\\widehat{\\tau} = \\tau(\\widehat{\\theta}) = k \\widehat{\\theta}(1 - \\widehat{\\theta})^{k-1} = k \\left( \\frac{\\sum^n_{i=1} X_i}{nk} \\right) \\left( 1 - \\frac{\\sum^n_{i=1} X_i}{nk} \\right)^{k-1} = \\bar{X}_n \\left(1 - \\frac{1}{k} \\bar{X}_n \\right)^{k-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3e)\n",
    "\n",
    "The weak law of large numbers (WLLN) states that a sample mean $\\bar{X}_n$ of IID random variables $X_1..., X_n$ converges in probability to their mean $\\mathbb{E}[X_i] = \\mu$, $\\bar{X}_n \\overset{p}{\\rightarrow} \\mu$. Hence we have that:\n",
    "\n",
    "$$\\bar{X}_n \\overset{p}{\\rightarrow} k \\theta$$\n",
    "\n",
    "because the $X_i$ are Binomial distributed with mean $\\mathbb{E}[X_i] = k\\theta$. Noting that $\\widehat{\\tau} = h(\\bar{X}_n) = \\bar{X}_n \\left(1 - \\frac{1}{k} \\bar{X}_n \\right)^{k-1}$, that $h(\\cdot)$ is a polynomial of degree $k$ and continuous, we have via the continuous mapping theorem:\n",
    "\n",
    "$$ h(\\bar{X}_n) \\overset{p}{\\rightarrow} h(k \\theta) = k\\theta(1 - \\theta)^{k-1} = \\tau(\\theta)$$\n",
    "\n",
    "Hence $\\widehat{\\tau} \\overset{p}{\\rightarrow} \\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3f)\n",
    "\n",
    "Under appropriate regularity conditions and assuming this for the entirety of the answer, the maximum likelihood estimator $\\widehat{\\theta}$ is asymptotically Normal, that is:\n",
    "\n",
    "$$\\sqrt{n}(\\widehat{\\theta} - \\theta) \\overset{d}{\\rightarrow} N \\left(0, \\frac{1}{I(\\theta)} \\right)$$\n",
    "\n",
    "where $I(\\theta)$ is the Fisher information for a single data point $X_i$.\n",
    "\n",
    "We can use the delta method to yield the asymptotic distribution of smooth functions of estimators with known limiting distribution, and it states:\n",
    "\n",
    "$$\\frac{\\widehat{\\tau} - \\tau}{\\text{se}(\\tau)} \\overset{d}{\\rightarrow} N(0, 1) \\implies \\tau(\\widehat{\\theta}) \\overset{d}{\\rightarrow} N \\left(\\tau(\\theta), \\frac{\\tau'(\\theta)^2}{I_n(\\theta)} \\right)$$\n",
    "\n",
    "where $\\text{se}(\\cdot)$ is the (asymptotic) standard error, and $I_n(\\theta)$ is the Fisher information for $n$ data points $X_1,...,X_n$.\n",
    "\n",
    "Computing $\\tau'(\\theta)^2$:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\tau'(\\theta)^2 &= \\left\\{ \\frac{\\partial}{\\partial \\theta} k \\theta (1 - \\theta)^{k-1} \\right\\}^2 \\\\\n",
    "&= \\left\\{-k\\theta(k-1)(1 - \\theta)^{k-2} + k(1 - \\theta)^{k-1} \\right\\}^2 \\\\\n",
    "&= \\left\\{k(1- \\theta)^{k-2}(1 - k \\theta) \\right\\}^2\n",
    "\\end{align}$$\n",
    "\n",
    "Now the Fisher information $I_n(\\theta)$ is attained by computing negative expectations of the second derivative of the log-likelihood, that is $I_n(\\theta) = - \\mathbb{E}_{\\theta}[l''(\\theta)]$. Using our results from 3d), and setting $S = \\sum^n_{i=1} x_i$ we first compute $l''(\\theta)$:\n",
    "\n",
    "$$\\begin{align}\n",
    "l''(\\theta) &= \\frac{\\partial}{\\partial \\theta} l'(\\theta) \\\\\n",
    "&= \\frac{\\partial}{\\partial \\theta} \\frac{(1 - \\theta)S - \\theta(nk - S)}{\\theta(1- \\theta)} \\\\\n",
    "&= \\frac{\\partial}{\\partial \\theta} \\frac{S - \\theta nk}{\\theta(1 - \\theta)} \\\\\n",
    "&= \\frac{-nk \\theta ( 1 - \\theta) - (S - \\theta nk)(1 - 2 \\theta)}{\\theta (1 - \\theta)} \\\\\n",
    "\\implies l''(\\theta) &= \\frac{- \\theta^2nk + 2 \\theta S - S}{[\\theta(1- \\theta)]^2}\n",
    "\\end{align}$$\n",
    "\n",
    "Taking negative expectations, we have that the Fisher information is:\n",
    "\n",
    "$$I_n(\\theta) = -\\mathbb{E}[l''(\\theta)] = -\\frac{1}{[\\theta(1-\\theta)]^2} \\mathbb{E}[-\\theta^2 nk + 2\\theta S - S] = \\frac{\\theta^2 nk - 2\\theta^2 nk + \\theta nk}{[\\theta(1- \\theta)]^2} = \\frac{nk}{\\theta(1 - \\theta)} $$\n",
    "\n",
    "Where we have used the fact that $\\mathbb{E}[S] = \\mathbb{E}[n\\bar{X}_n] = \\theta nk$.\n",
    "\n",
    "Hence the asymptotic variance of $\\tau(\\widehat{\\theta})$ is:\n",
    "\n",
    "$$\\frac{\\tau'(\\theta)^2}{I_n(\\theta)} = \\frac{\\left[k(1- \\theta)^{k-2}(1 - k \\theta) \\right]^2 \\theta(1 - \\theta)}{nk} = \\frac{k\\theta(1-\\theta)^{2k-3}(1 - k\\theta)^2}{n}$$\n",
    "\n",
    "And $\\widehat{\\tau}$ has the limiting distribution:\n",
    "\n",
    "$$\\widehat{\\tau} \\overset{d}{\\rightarrow} N \\left(k\\theta(1- \\theta)^{k-1}, \\frac{k\\theta(1-\\theta)^{2k-3}(1 - k\\theta)^2}{n} \\right)$$\n",
    "\n",
    "We can also estimate the asymptotic variance by evaluating it at $\\widehat{\\theta} = k^{-1}\\bar{X}_n$, yielding the following limiting distribution for $\\widehat{\\tau}$:\n",
    "\n",
    "$$\\widehat{\\tau} \\overset{d}{\\rightarrow} N \\left(k\\theta(1- \\theta)^{k-1}, \\frac{k\\bar{X}_n(1-k^{-1}\\bar{X}_n)^{2k-3}(1 - \\bar{X}_n)^2}{n} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Construct an example where $X_n \\overset{d} X$ and $Y_n \\overset{d}{\\rightarrow} Y$, but $X_n + Y_n$ does not converge in distribution to $X + Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Let $X_i \\sim N(\\theta_i, 1)$ for $i = 1, 2, ... , n$. Let $\\gamma = \\sum^n_{i=1} \\theta^2_i$. \n",
    "\n",
    "Find the maximum likelihood estimator $\\widehat{\\gamma}$. \n",
    "\n",
    "Let $L(\\gamma, \\widehat{\\gamma}) = (\\gamma - \\widehat{\\gamma})^2$. Find the risk of the maximum likelihood estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)\n",
    "\n",
    "We assume that the $X_i$ are independent; but it is clear that they are not identically distributed.\n",
    "\n",
    "Denoting $\\boldsymbol{\\theta} = (\\theta_1, ..., \\theta_n) \\in \\mathbb{R}^n$, we first compute the maximum likelihood estimator $\\widehat{\\boldsymbol{\\theta}} = (\\widehat{\\theta}_1, ..., \\widehat{\\theta}_n)$. The log-likelihood function is:\n",
    "\n",
    "$$l(\\boldsymbol{\\theta}) = \\log \\frac{1}{(2 \\pi)^{n/2}} \\exp \\left(- \\frac{1}{2} \\sum^n_{i=1} (x_i - \\theta_i)^2 \\right) = -\\frac{n}{2} \\log (2 \\pi) - \\frac{1}{2} \\sum^n_{i=1} (x_i - \\theta_i)^2$$\n",
    "\n",
    "Taking derivatives with respect to each $\\theta_i$, we have that:\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\theta_i} = \\frac{1}{2}2(x_i - \\theta_i) = x_i - \\theta_i = 0 \\implies \\widehat{\\theta}_i = x_i$$\n",
    "\n",
    "Hence we have that the maximum likelihood estimator $\\widehat{\\boldsymbol{\\theta}} = (x_1,...,x_n)$, which in this case is just the data that has been observed.\n",
    "\n",
    "Defining the scalar valued function $g: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, with $g(\\mathbf{u}) = \\mathbf{u}^T\\mathbf{u}$, we have that $\\gamma = g(\\boldsymbol{\\theta}) = \\boldsymbol{\\theta}^T\\boldsymbol{\\theta} = \\sum^n_{i=1} \\theta_i^2$. Hence denoting the data as $\\mathbf{x} = (x_1, ...., x_n)$, via equivariance of the maximum likelihood estimator, we have that:\n",
    "\n",
    "$$\\widehat{\\gamma} = g(\\widehat{\\boldsymbol{\\theta}}) = \\mathbf{x}^T\\mathbf{x} = \\sum^n_{i=1} x_i^2$$\n",
    "\n",
    "The risk function $R(\\gamma, \\widehat{\\gamma})$ is the expected value of the loss function $L(\\gamma, \\widehat{\\gamma})$:\n",
    "\n",
    "$$R(\\gamma, \\widehat{\\gamma}) = \\mathbb{E}_{\\gamma}[L(\\gamma, \\widehat{\\gamma})]$$\n",
    "\n",
    "Under $L_2$, or squared-error loss, $L(\\gamma, \\widehat{\\gamma}) = (\\gamma - \\widehat{\\gamma})^2$, the risk function is just the mean squared error (MSE), $\\mathbb{E}_{\\gamma}[(\\gamma - \\widehat{\\gamma})^2]$. As the mean-squared error is the sum of the variance $V$ and the squared bias $B^2$, we have:\n",
    "\n",
    "$$\\text{MSE} = B^2 + V = (\\mathbb{E}_{\\gamma}[\\widehat{\\gamma}] - \\gamma)^2 - \\text{Var}_{\\gamma}[\\widehat{\\gamma}]$$\n",
    "\n",
    "Computing the expectations of the estimator $\\widehat{\\gamma}$:\n",
    "\n",
    "$$\\mathbb{E}_{\\gamma}[\\widehat{\\gamma}] = \\mathbb{E}\\left[ \\sum^n_{i=1} x_i^2 \\right] = \\sum^n_{i=1} \\mathbb{E}[x_i^2] = \\sum^n_{i=1} \\text{Var}(x_i) + \\mathbb{E}[x_i]^2 = \\sum^n_{i=1} (1 + \\theta_i)^2 = n + \\sum^n_{i=1} \\theta_i^2$$\n",
    "\n",
    "And so the squared bias is:\n",
    "\n",
    "$$B^2 = \\left( n + \\sum^n_{i=1} \\theta_i^2 - \\sum^n_{i=1} \\theta_i^2 \\right)^2 = n^2$$\n",
    "\n",
    "Computing the variance of the estimator relies on the observation that it is the sum of squares of $n$ independent Normally distributed random variables, each with individual mean $\\theta_i$ and unit variance. Hence the estimator will have a non-central Chi-squared distribution with $n$ degrees of freedom and non-centrality parameter $\\lambda = \\sum^n_{i=1} \\theta_i^2$.\n",
    "\n",
    "Stating this in a multivariate form, we have that for a multivariate Normal random vector $\\mathbf{X} = (X_1, ... X_n)$ with mean $\\boldsymbol{\\theta} = (\\theta_1,...,\\theta_n)$ and isotropic covariance $\\Sigma = \\sigma^2I$, it is the case that:\n",
    "\n",
    "$$\\frac{\\lVert \\mathbf{X} \\rVert^2_2}{\\sigma^2} = \\frac{\\mathbf{X}^T\\mathbf{X}}{\\sigma^2} \\sim {{\\chi}}_n^2 \\left( \\frac{\\boldsymbol{\\theta}^T\\boldsymbol{\\theta}}{\\sigma^2}\\right)$$\n",
    "\n",
    "Setting $\\sigma^2 = 1$ yields the non-centrality parameter $\\lambda = \\boldsymbol{\\theta}^T\\boldsymbol{\\theta} = \\sum^n_{i=1} \\theta^2_i$. The variance of this distribution is $2(n + 2\\lambda)$, and so we have that the risk function is:\n",
    "\n",
    "$$R(\\gamma, \\widehat{\\gamma}) = n^2 + 2\\left(n + 2\\sum^n_{i=1} \\theta^2_i\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Let $X_1, ... , X_n \\sim \\text{Uniform}(0, 2)$. Let \n",
    "\n",
    "$$W_n = \\frac{1}{n} \\sum^n_{i=1} X^2_i$$\n",
    "\n",
    "a) Show that there is a number $\\mu$ such that $W_n$ converges in quadratic mean to $\\mu$.\n",
    "\n",
    "b) Show that $W_n$ converges in probability to $\\mu$.\n",
    "\n",
    "c) What is the limiting distribution of $\\sqrt{n}(W^2_n - \\mu^2)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6a)\n",
    "\n",
    "The PDF of $X_i \\sim \\text{Uniform}(0,2)$ is:\n",
    "\n",
    "$$f_{X_i}(x_i) = \n",
    "\\begin{cases}\n",
    "\\frac{1}{2} &0 \\leq x_i \\leq 2 \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "To show that $W_n$ converges in quadratic mean to $\\mu$, that is, $W_n \\overset{\\text{q.m.}}{\\rightarrow} \\mu$, it is necessary to show that as $n \\rightarrow \\infty$:\n",
    "\n",
    "$$\\mathbb{E}[(W_n - \\mu)^2] \\rightarrow 0$$\n",
    "\n",
    "Defining the transformation $Y = r(X) = X^2$, notice that we can express $W_n$ as a sample mean $\\bar{Y}_n$ of the transformed IID random variables $Y_1,...,Y_n$:\n",
    "\n",
    "$$W_n = \\frac{1}{n} \\sum^n_{i=1} Y_i = \\bar{Y}_n$$\n",
    "\n",
    "We now need to ascertain the properties of $Y$ by computing its mean and variance. We evaluate the CDF and PDF in order to do so for practice, but a quicker method would be to use the law of the unconscious statistician:\n",
    "\n",
    "The CDF of $Y$ can be found by considering:\n",
    "\n",
    "$$F_Y(y) = P(r(X) \\leq y) = P(\\{x: x^2 \\leq y\\}) = \\int_{A_y} f_X(x) dx$$\n",
    "\n",
    "Where the integral is over the set $A_y = \\{x: r(x) \\leq y) \\}$. The support of $X$ is $0 \\leq x \\leq 2$, so we consider $0 \\leq y \\leq 4$. Hence we have that:\n",
    "\n",
    "$$F_Y(y) = P(\\left \\{ x: x^2 \\leq y \\} \\right) P(\\left\\{x: \\sqrt{y} \\leq x \\leq \\sqrt{y} \\right\\})= \\int_{\\left\\{x: \\sqrt{y} \\leq x \\leq \\sqrt{y} \\right\\}} f_X(x) dx = F_X(\\sqrt{y}) - F_X(-\\sqrt{y}) = \\frac{1}{2}\\sqrt{y} - \\left(- \\frac{1}{2}\\sqrt{y} \\right) = \\sqrt{y}$$\n",
    "\n",
    "And so the CDF of $Y$ is:\n",
    "\n",
    "$$F_Y(y) = \n",
    "\\begin{cases}\n",
    "0 &y < 0 \\\\\n",
    "\\sqrt{y} &0 \\leq y \\leq 4 \\\\\n",
    "1 &y>4 \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "And taking derivatives, we have the following PDF of $Y$:\n",
    "\n",
    "$$f_Y(y) = \n",
    "\\begin{cases}\n",
    "\\frac{1}{2 \\sqrt{y}} &0 < y < 4 \\\\\n",
    "0 &\\text{otherwise} \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "We have that the mean and variance are given by:\n",
    "\n",
    "$$\\mathbb{E}[$$\n",
    "\n",
    "Defining the transformation $Y = r(X) = X^2$, notice that we can express $W_n$ as a sample mean $\\bar{Y}_n$ of the transformed IID random variables $Y_1,...,Y_n$:\n",
    "\n",
    "$$W_n = \\frac{1}{n} \\sum^n_{i=1} Y_i = \\bar{Y}_n$$\n",
    "\n",
    "We now want to compute the mean and variance of $Y$. The $\\text{Uniform}(a,b)$ distribution has mean $(a + b)/2$ and variance $(b-a)^2/12$, and as the mean of $Y$ is also the 2nd moment of $X$, that is, $\\mathbb{E}[Y] = \\mathbb{E}[X^2]$, we have that $\\mathbb{E}[Y] = \\text{Var}[X] + \\mathbb{E}[X]^2 = 4/12 + 1^2 = 4/3 = \\mu_Y$. \n",
    "\n",
    "We compute the variance of the $Y$ using the law of the unconscious statistician:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{Var}[Y] &= \\mathbb{E}[(Y - \\mu_Y)^2] \\\\  \n",
    "&= \\mathbb{E}[Y^2 - 2\\mu_YY + \\mu_Y^2] \\\\ \n",
    "&= \\mathbb{E}[X^4 - 2\\mu_YX^2 + \\mu_Y^2] \\\\ \n",
    "&= \\int^2_0 (x^4 - 2\\mu_Yx^2 + \\mu_Y^2)f_X(x) dx \\\\\n",
    "&= \\int^2_0 \\frac{1}{2}x^4 - \\mu_Yx^2 + \\frac{1}{2}\\mu_Y^2 dx \\\\\n",
    "&= \\left[ \\frac{1}{10} x^5 - \\frac{\\mu_Y}{3}x^3 - \\frac{1}{2}\\mu_Y^2x \\right]^2_0 \\\\\n",
    "&= \\frac{1}{10}(2)^5 - \\frac{4}{9}(2)^3 - \\frac{1}{2}\\frac{16}{9}(2) = \\frac{64}{45} = \\sigma^2_Y\n",
    "\\end{align}$$\n",
    "\n",
    "Now we have that:\n",
    "\n",
    "$$\\mathbb{E}[(W_n - \\mu)^2] = \\mathbb{E}[(\\bar{Y}_n - \\mu_Y)^2] = \\mathbb{E}[\\bar{Y}_n^2 - 2\\mu_Y\\bar{Y}_n + \\mu_Y^2] = \\mathbb{E}[\\bar{Y}_n^2] - 2 \\mu_Y \\mathbb{E}[\\bar{Y}_n] + \\mu_Y^2$$ \n",
    "\n",
    "As $\\bar{Y}_n$ is a sample mean, its mean, variance and second moment can be specified in terms of the underlying random variables $Y_i$, and so we have that:\n",
    "\n",
    "$$\\mathbb{E}[\\bar{Y}_n] = \\mu_Y, \\quad \\text{Var}[\\bar{Y}_n] = \\frac{\\sigma^2_Y}{n}, \\quad \\mathbb{E}[\\bar{Y}_n^2] = \\frac{\\sigma^2_Y}{n} + \\mu_Y^2$$\n",
    "\n",
    "Hence we have that:\n",
    "\n",
    "$$\\mathbb{E}[(W_n - \\mu)^2] = \\mathbb{E}[(\\bar{Y}_n - \\mu_Y)^2] = \\left(\\frac{\\sigma^2_Y}{n} + \\mu_Y^2 \\right) - 2 \\mu_Y^2 + \\mu_Y^2 = \\frac{64}{45n} \\rightarrow 0$$\n",
    "\n",
    "as $n \\rightarrow \\infty$, which proves the required result, $W_n \\overset{\\text{q.m.}}{\\rightarrow} \\mu$ and where the number $\\mu = \\mu_Y = 4/3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6b)\n",
    "\n",
    "Because of the way we have set up the problem, the convergence in probability result, $W_n \\overset{p}{\\rightarrow} \\mu$ is a consequence of the weak law of large numbers (WLLN) applied to the sample mean $\\bar{Y}_n$, so that we have:\n",
    "\n",
    "$$\\bar{Y}_n \\overset{p}{\\rightarrow} \\mu_Y = \\frac{4}{3}$$\n",
    "\n",
    "Alternatively, can use Hoeffding's inequality, which states that for bounded random variables $a \\leq Y_i \\leq b$ with mean $\\mu_Y$:\n",
    "\n",
    "$$P(|\\bar{Y}_n - \\mu_Y | > \\epsilon) \\leq 2\\exp \\left( \\frac{-2n\\epsilon^2}{(b-a)^2} \\right)$$\n",
    "\n",
    "Hence we have that\n",
    "\n",
    "$$P(|W_n - \\mu | > \\epsilon) = P(|\\bar{Y}_n - \\mu_Y | > \\epsilon) \\leq 2 \\exp \\left( \\frac{-n \\epsilon^2}{8} \\right) \\longrightarrow 0$$\n",
    "\n",
    "as $n \\rightarrow \\infty$ and so $W_n \\overset{p}{\\rightarrow} \\mu$ at a rate of ***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6c)\n",
    "\n",
    "As the sample mean $Y_n$ is asymptotically Normal with mean $\\mu_Y$ and variance $\\sigma^2_Y / n$, we define the function $g(u) := u^2$, which is smooth, and using the delta method, which states:\n",
    "\n",
    "$$\\sqrt{n}(g(\\bar{Y}_n) - g(\\mu_Y) \\overset{d}{\\rightarrow} N \\left(0, \\sigma^2_Y g'(\\mu_Y)^2 \\right)$$\n",
    "\n",
    "We have that:\n",
    "\n",
    "$$g'(\\mu_Y)^2 = (2 \\mu_Y)^2 \\implies \\sqrt{n}(W^2_n - \\mu^2) \\overset{d}{\\rightarrow} N(0, \\left(2 \\mu_Y \\sigma_Y)^2 \\right)$$\n",
    "\n",
    "And hence the $\\sqrt{n}(W^2_n - \\mu)$ is asymptotically Normal with mean $0$ and variance $(2 \\mu_Y \\sigma_Y)^2$, where $\\mu_Y = \\mu = 4/3$ and $\\sigma^2_Y = 64/45$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Let $X_1, ... X_n \\sim \\text{Bernoulli}(p)$.\n",
    "\n",
    "a) Let $T = X_3$. Show that $T$ is not sufficient.\n",
    "\n",
    "b) Show that $U = \\sum^n_{i=1} X^2_i$ is minimal sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7a), 7b)\n",
    "\n",
    "We first show that $U = \\sum^n_{i=1} X_i^2$ is minimal sufficient before showing that $T = X_3$ is not sufficient.\n",
    "\n",
    "For $X_i \\sim \\text{Bernoulli}(p)$, we have that $P(X_i = 1) = p$ and $P(X_i = 0) = (1-p)$ with PMF $f_{X_i}(x_i) = p^{x_i}(1-p)^{1-x_i}$.\n",
    "\n",
    "Using similar arguments to 3a), computing $R(x^n, y^n ; p)$ yields:\n",
    "\n",
    "$$R(x^n, y^n ; p) = \\frac{\\prod^n_{i=1} p^{x_i} (1-p)^{1-x_i}}{\\prod^n_{i=1} p^{y_i} (1-p)^{1-y_i}} = \\frac{p^{\\sum^n_{i=1} x_i} (1-p)^{n - \\sum^n_{i=1} x_i}}{p^{\\sum^n_{i=1} y_i} (1-p)^{n - \\sum^n_{i=1} y_i}} = \\frac{p^{\\sum^n_{i=1} x_i^2} (1-p)^{n - \\sum^n_{i=1} x_i^2}}{p^{\\sum^n_{i=1} y_i^2} (1-p)^{n - \\sum^n_{i=1} y_i^2}}$$\n",
    "\n",
    "Where we have used the fact that because $X_i$ is a Bernoulli random variable, $X^2_i = X_i$ in going from the second to third equality.\n",
    "\n",
    "If we set $\\sum^n_{i=1} x_i^2 = \\sum^n_{i=1} y_i^2$, then $R(x^n, y^n ; p) = 1$ which does not depend on the parameter $p$. Furthermore, the only way in which $R(x^n, y^n ; p)$ does not depend on $p$ is when it is equal to $1$, and this occurs when $\\sum^n_{i=1} x_i^2 = \\sum^n_{i=1} y_i^2$.\n",
    "\n",
    "Hence $U(X^n) =  \\sum^n_{i=1} X_i^2$ is a minimal sufficient statistic. \n",
    "\n",
    "Using similar arguments to 3b), we have that $T = X_3$ is not sufficient because we cannot express a known minimal sufficient statistic $U = \\sum^n_{i=1} X_i^2$ as a function of $T$. That is, there exists no function $g$ such that $U = g(T)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Let:\n",
    "\n",
    "$$X_1, ... X_n \\sim \\frac{1}{2}N(0, 1) + \\frac{1}{2} N(\\theta, 1)$$\n",
    "\n",
    "In other words, with probability $1/2$, $X_i$ is drawn from $N(0,1)$ and with probabiltiy $1/2$, $X_i$ is drawn from $N(\\theta, 1)$.\n",
    "\n",
    "a) Find the method of moments estimator $\\widehat{\\theta}$ of $\\theta$.\n",
    "\n",
    "b) Find the mean squared error of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8a)\n",
    "\n",
    "It is necessary to note that each $X_i$ is a mixture of Normal distributions. We denote the mixture component Normal probability densities as $f_{X_{i1}}(x_i) = N(0,1)$ and $f_{X_{i2}}(x_i) = N(\\theta, 1)$, and the mixture weights are $(1/2, 1/2)$.\n",
    "\n",
    "Hence the PDF of $X_i$ is:\n",
    "\n",
    "$$f_{X_i}(x_i) = \\frac{1}{2}f_{X_{i1}}(x_i) + \\frac{1}{2}  f_{X_{i2}}(x_i)=  \\frac{1}{2} \\left(\\frac{1}{\\sqrt{2 \\pi}} e^{-x_i^2/2} \\right) + \\frac{1}{2} \\left(\\frac{1}{\\sqrt{2 \\pi}} e^{-(x_i - \\theta_i)^2 / 2} \\right)$$\n",
    "\n",
    "To compute the method of moments estimator $\\widehat{\\theta}$ of $\\theta$, we equate the $k$th theoretical moment $\\mathbb{E}[X^k]$ with the $k$th sample moment $m_k = \\frac{1}{n} \\sum^n_{i=1} X_i^k$. As there is only one parameter $\\theta$ for which we are looking to compute an estimator, this means we only require the 1st moment i.e. the expectation and sample mean.\n",
    "\n",
    "Computing the mean, we have:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}[X_i] &= \\int^{\\infty}_{\\infty} x_i f_{X_i}(x_i) dx_i \\\\\n",
    "&= \\int^{\\infty}_{\\infty} x_i \\left( \\frac{1}{2} f_{X_{i1}}(x_i) + \\frac{1}{2} f_{X_{i2}}(x_i) \\right) dx_i  \\\\\n",
    "&= \\frac{1}{2} \\int^{\\infty}_{\\infty} x_i f_{X_{i1}}(x_i) dx_i  + \\frac{1}{2} \\int^{\\infty}_{\\infty} x_i f_{X_{i2}}(x_i) dx_i \\\\\n",
    "&= \\frac{1}{2} \\mathbb{E}[X_{i1}] + \\frac{1}{2} \\mathbb{E}[X_{i2}] \\\\\n",
    "&= \\frac{1}{2}(0) + \\frac{1}{2}(\\theta) \\\\\n",
    "&= \\frac{1}{2}\\theta\n",
    "\\end{align}$$\n",
    "\n",
    "Equating $\\mathbb{E}[X_i]$ with the sample mean $m_1 = \\bar{X}_n$:\n",
    "\n",
    "$$\\bar{X}_n = \\frac{1}{2} \\theta$$\n",
    "\n",
    "Hence the method of moments estimator is:\n",
    "\n",
    "$$\\widehat{\\theta} = 2 \\bar{X}_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8b)\n",
    "\n",
    "The mean squared error (MSE) of an estimator is the sum of its variance and its squared bias:\n",
    "\n",
    "$$\\text{MSE} = \\mathbb{E}_{\\theta}[(\\widehat{\\theta} - \\theta)^2] = B^2 + V = \\left(\\mathbb{E}_{\\theta}[\\widehat{\\theta}] - \\theta)^2 + \\text{Var}_{\\theta}[\\widehat{\\theta}] \\right)$$\n",
    "\n",
    "Suppressing $\\theta$ from the expectation, the squared bias is:\n",
    "\n",
    "$$B^2 = \\left( \\mathbb{E}[2 \\bar{X}_n] - \\theta \\right)^2 = \\left( 2 \\mathbb{E}[\\bar{X}_n] - \\theta \\right)^2 = \\left(2 \\cdot \\frac{1}{2} \\theta - \\theta \\right)^2 = 0 \\implies B = 0$$\n",
    "\n",
    "Hence the method of moments estimator for this particular mixture of Normal distributions is unbiased.\n",
    "\n",
    "In order to compute the variance of $\\widehat{\\theta}$, we compute the 2nd theoretical moment of $X_i$, and in addition the variance of the latter. Following similar calculations in 8a) we have that:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}[X_i^2] &= \\frac{1}{2} \\mathbb{E}[X^2_{i1}] + \\frac{1}{2} \\mathbb{E}[X^2_{i2}] \\\\\n",
    "&= \\frac{1}{2} \\left( \\text{Var}[X_{i1}] + \\mathbb{E}[X_{i1}]^2 \\right) + \\frac{1}{2} \\left( \\text{Var}[X_{i2}] + \\mathbb{E}[X_{i2}]^2 \\right) \\\\\n",
    "&= \\frac{1}{2}\\left( 1 + 0^2 \\right) + \\frac{1}{2} \\left( 1 + \\theta^2 \\right) \\\\\n",
    "&= 1 + \\frac{1}{2} \\theta^2\n",
    "\\end{align}$$\n",
    "\n",
    "Hence the variance of $X_i$ is:\n",
    "\n",
    "$$\\text{Var}[X_i] = \\mathbb{E}[X_i^2] - \\mathbb{E}[X_i]^2 = \\left( 1 + \\frac{1}{2} \\theta^2 \\right) - \\left(\\frac{\\theta}{2}\\right)^2 = 1 + \\frac{1}{4} \\theta^2 = \\sigma^2$$\n",
    "\n",
    "As $\\text{Var}[\\bar{X}_n] = \\sigma^2/n$, we have the variance of the estimator $\\widehat{\\theta}$:\n",
    "\n",
    "$$\\text{Var}[2 \\bar{X}_n] = 4 \\text{Var}[\\bar{X}_n] = \\frac{4\\sigma^2}{n} = \\frac{4 \\left(1 + \\frac{\\theta^2}{4} \\right)}{n} = \\frac{4 + \\theta^2}{n} = \\text{MSE}$$\n",
    "\n",
    "And because the estimator is unbiased, $B = 0$, the mean squared error is equal to the variance of the estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Let $X_1, ... , X_n \\sim N(\\theta, 1)$. Let $\\tau = e^{\\theta} + 1$.\n",
    "\n",
    "a) Consider some loss function $L(\\tau, \\widehat{\\tau})$. Define what it means for an estimator to be the minimax estimator for $\\tau$.\n",
    "\n",
    "b) Let $\\pi$ be some prior for $\\theta$. Find the Bayes estimator for $\\tau$ under the loss $L(\\tau, \\widehat{\\tau}) = (\\widehat{\\tau} - \\tau)^2 / \\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9a)\n",
    "\n",
    "The minimax estimator $\\widetilde{\\tau}$ minimises the maximum risk. That is, out of all possible estimators $\\widehat{\\tau}$, each with their own maximum risk $\\bar{R}(\\widehat{\\tau})$, where the maximum risk is with respect to the parameter $\\tau$, the maximum risk of the minimax estimator $\\bar{R}(\\widetilde{\\tau})$ is the lowest out of all possible estimators $\\widehat{\\tau}$. \n",
    "\n",
    "Formally $\\widetilde{\\tau}$ is the minimax estimator if:\n",
    "\n",
    "$$\\bar{R}(\\widetilde{\\tau}) = \\underset{\\tau}{\\sup} R(\\tau, \\widetilde{\\tau}) = \\underset{\\widehat{\\tau}}{\\inf} \\underset{\\tau}{\\sup} R(\\tau, \\widehat{\\tau})$$\n",
    "\n",
    "And that is with respect to a particular specification of the loss function $L(\\tau, \\widehat{\\tau})$. It can be viewed as selecting an estimator on the basis of its worst case behaviour.\n",
    "\n",
    "There is some machinery to unpack here, and we take this opportunity to do so whilst clarifying the role of the loss function, risk function, maximum risk, and minimax risk.\n",
    "\n",
    "A loss function $L(\\tau, \\widehat{\\tau})$ is a measure of the \"quality\" of an estimator, and we can endow it with particular functional forms to capture how we want to \"penalise\" an estimator $\\widehat{\\tau}$ deviating from the parameter $\\tau$, or how we wish to assign numerical values to discrepancies between $\\widehat{\\tau}$ and $\\tau$.\n",
    "\n",
    "The loss function is a function of both the unknown parameter $\\tau$, and the estimator $\\widehat{\\tau}(X_1, ..., X_n)$. The loss function is a random quantity because it is dependent on the data $X_1, ... , X_n$ through the estimator. Formally the loss function is mapping from $T \\times T \\rightarrow \\mathbb{R}$. In the context of statistical inference, where we have observed the data as the outcome of some experiment, that is $X_1 = x_1, ... , X_n = x_n$, our estimator becomes an estimate $\\widehat{\\tau}(x_1, ..., x_n)$. However, as we do not know the value of the loss function due to the presence of the unknown parameter $\\tau$, we cannot compute it.\n",
    "\n",
    "Having specified a loss function, the risk function $R(\\tau, \\widehat{\\tau})$ is the average loss incurred by the estimator $\\widehat{\\tau}$, that is, $\\mathbb{E}_{\\tau}[L(\\tau, \\widehat{\\tau})]$. We are interested in this quantity because we are interested in the performance of an estimator not for one particular dataset $X^n = x^n$, but all possible $X^n$. By taking expectations with respect to the joint distribution that generated the data, i.e. $p(X_1, ..., X_n; \\theta)$, we average out the data, and for a particular estimator $\\widehat{\\tau}$, the risk function $R(\\cdot, \\widehat{\\tau})$ is a function of the parameter, mapping from $T \\rightarrow \\mathbb{R}^+$.\n",
    "\n",
    "Given a particular estimator $\\widehat{\\tau}$, we cannot compute the value of the risk function, as the true parameter $\\tau$ is unknown, and also fixed. However, as it is unknown we now imagine that the the parameter $\\tau$ can vary within the range defined by the parameter space $T$, and specify the risk function in terms of $\\tau$.\n",
    "\n",
    "Hence for a number of different estimators $\\widehat{\\tau}_1, ..., \\widehat{\\tau}_n$, each will yield an individual risk function $R_i(\\cdot, \\widehat{\\tau}_i)$ whose output varies as a function of the parameter $\\tau$ over the parameter space $T$. In order to compare risk functions of different estimators, we rely on one-number summaries such as the maximum risk $\\bar{R}(\\widehat{\\tau}) = \\sup_\\tau R_i(\\tau, \\widehat{\\tau}_i)$, which is a maximum with respect to the parameter $\\tau$, for a particular estimator $\\widehat{\\tau}_i$.\n",
    "\n",
    "The minimax estimator $\\widetilde{\\tau}$ is that estimator whose maximum risk $\\bar{R}(\\widetilde{\\tau})$ is the lowest amongst all possible estimators $\\widehat{\\tau}$, and so $\\bar{R}(\\widetilde{\\tau}) = \\inf_{\\widehat{\\tau}} \\sup_{\\tau} R(\\tau, \\widehat{\\tau})$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) Let $X_1, ... X_n \\sim \\text{Normal} (\\theta, 1)$. Suppose that $\\theta \\in \\{ 1, -1 \\}$. In other words, $\\theta$ can only take two possible values.\n",
    "\n",
    "a) Find a minimal sufficient statistic.\n",
    "\n",
    "b) Find the maximum likelihood estimator. Is the maximum likelihood estimator a sufficient statistic? \n",
    "\n",
    "c) Find the risk function using the loss function\n",
    "\n",
    "$$L(\\theta, \\widehat{\\theta}) = \n",
    "\\begin{cases}\n",
    "1 \\quad &\\text{if} \\space \\theta \\neq \\widehat{\\theta} \\\\\n",
    "0 \\quad &\\text{if} \\space \\theta = \\widehat{\\theta} \\\\\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10a)\n",
    "\n",
    "A reasonable guess for a minimal sufficient statistic would be $T = \\bar{X}_n$ - the sample mean.\n",
    "\n",
    "Following the same arguments as 3a), we compute the ratio of Normal densities with mean $\\theta$ and unit variance for two datasets $X^n = x^n$ and $Y^n = y^n$:\n",
    "\n",
    "$$R(x^n, y^n; \\theta) = \\frac{(2 \\pi)^{-n/2} \\exp \\left(-\\frac{1}{2} \\sum^n_{i=1}(x_i - \\theta )\\right)^2}{(2 \\pi)^{-n/2} \\exp\\left(-\\frac{1}{2} \\sum^n_{i=1}(y_i - \\theta)^2 \\right)} = \\frac{\\exp \\left(-\\frac{1}{2} \\sum^n_{i=1}(x_i - \\bar{x})^2 \\right) \\exp \\left(-\\frac{n}{2} (\\bar{x} - \\theta)^2 \\right)}{\\exp \\left(-\\frac{1}{2} \\sum^n_{i=1}(y_i - \\bar{y})^2 \\right) \\exp \\left(-\\frac{n}{2} (\\bar{y} - \\theta)^2 \\right)}$$\n",
    "\n",
    "Now if we set $\\bar{x} = \\bar{y}$, we have that:\n",
    "\n",
    "$$R(x^n, y^n; \\theta) = \\frac{\\exp \\left(-\\frac{1}{2} \\sum^n_{i=1}(x_i - \\bar{x})^2 \\right)}{\\exp \\left(-\\frac{1}{2} \\sum^n_{i=1}(y_i - \\bar{x})^2 \\right)} = c$$\n",
    "\n",
    "i.e. $R$ is constant, and does not depend on $\\theta$. To show the reverse implication holds true, i.e. if $R$ does not depend on $\\theta$, then it must be the case that $\\bar{x} = \\bar{y}$, we compute derivatives of $R$ with respect to $\\theta$:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\theta} R(x^n, y^n; \\theta) &= \\frac{\\partial}{\\partial \\theta} \\left[c \\cdot \\frac{\\exp \\left(-\\frac{n}{2} (\\bar{x} - \\theta)^2 \\right)}{\\exp \\left(-\\frac{n}{2} (\\bar{y} - \\theta)^2 \\right)} \\right] \\\\ \n",
    "&= c \\cdot \\frac{\\exp \\left(-\\frac{n}{2} (\\bar{x} - \\theta)^2 \\right) n(\\bar{x} - \\theta) \\exp \\left(-\\frac{n}{2} (\\bar{y} - \\theta)^2 \\right) - \\exp \\left(-\\frac{n}{2} (\\bar{x} - \\theta)^2 \\right) \\exp \\left(-\\frac{n}{2} (\\bar{y} - \\theta)^2 \\right) n(\\bar{y} - \\theta)}{\\exp \\left(-\\frac{n}{2} (\\bar{y} - \\theta)^2 \\right)^2}\n",
    "\\end{align}$$\n",
    "\n",
    "If $R$ does not depend on $\\theta$, then this derivative must be 0 and rearranging we have that:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta} R(x^n, y^n; \\theta) = 0 \\implies n(\\bar{x} - \\theta) = n(\\bar{y} - \\theta) \\implies \\bar{x} = \\bar{y}$$\n",
    "\n",
    "Hence $T(X^n) = \\bar{X}_n$ is a minimal sufficient statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10b)\n",
    "\n",
    "We show that the maximum likelihood estimator $\\widehat{\\theta}$ of the mean $\\theta$ in this setting is the sample mean, $\\widehat{\\theta} = \\bar{X}_n$.\n",
    "\n",
    "Computing the log-likelihood by taking the product of Normal densities with mean $\\theta$ and unit variance, and evaluating for a particular realisation of the data $X^n = x^n$, we have that $l(\\theta)$:\n",
    "\n",
    "$$l(\\theta) = -\\frac{n}{2} \\log (2 \\pi) -\\frac{1}{2}  \\sum^n_{i=1} (x_i - \\theta)^2$$\n",
    "\n",
    "Taking derivatives with respect to $\\theta$, setting to 0 and solving for $\\theta$, we have that:\n",
    "\n",
    "$$l'(\\theta) = 0 \\implies \\sum^n_{i=1} (x_i - \\theta) = 0 \\implies \\widehat{\\theta} = \\frac{1}{n} \\sum^n_{i=1} X_i = \\bar{X}_n$$\n",
    "\n",
    "As we established in the previous section that the sample mean is minimal sufficient in this case, the maximum likelihood estimator $\\widehat{\\theta} = \\bar{X}_n$ is also minimal sufficient. \n",
    "\n",
    "If we did not have the previous result, and were interested in whether the the maximum likelihood estimator $\\widehat{\\theta} = \\bar{X}_n$ was sufficient, observe the following factorisation of the joint PDF:\n",
    "\n",
    "$$f_{X^n}(x^n; \\theta) = (2 \\pi)^{-n/2} \\exp \\left( - \\frac{1}{2} \\sum^n_{i=1} (x_i - \\theta)^2 \\right) = \\underbrace{(2 \\pi)^{-n/2} \\exp \\left( - \\frac{1}{2} \\sum^n_{i=1} (x_i - \\bar{x})^2  \\right)}_{h(x^n)} \\underbrace{\\exp \\left(-\\frac{n}{2}(\\bar{x} - \\theta)^2 \\right)}_{g(T(x^n), \\theta)}$$\n",
    "\n",
    "We have factorised the joint PDF into a function $h(x^n)$ that depends only on the data, and a function $g(T(x^n), \\theta)$ that depends on the parameter $\\theta$ and the data $x^n$, but with the depenendence on the latter only through a statistic $T(x^n)$. So by the Fisher-Neyman factorisation theorem, the maximum likelihood estimator is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) Let $X_i \\sim \\text{Bernoulli}(p_i)$ for $i = 1, 2, ..., n$. The observations are independent but each observation has a different mean. The unknown parameter $p = (p_1, ... , p_n)$.\n",
    "\n",
    "a) Let $\\psi = \\sum^n_{i=1} p_i$. Find the maximum likelihood estimator of $\\psi$.\n",
    "\n",
    "b) Find the mean squared error (MSE) of the maximum likelihood estimator of $\\psi$.\n",
    "\n",
    "c) Suppose we use the following prior distribution:\n",
    "\n",
    "$$\\pi(p_1, ... p_n) = 1$$\n",
    "\n",
    "Find the Bayes estimator of $\\psi$.\n",
    "\n",
    "Hint: Recall that the Beta $(\\alpha, \\beta)$ density is:\n",
    "\n",
    "$$\\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} p^{\\alpha - 1}(1-p)^{\\beta - 1}$$\n",
    "\n",
    "If $W \\sim \\text{Beta}(\\alpha, \\beta)$ then $\\mathbb{E}[W] = \\alpha/ (\\alpha + \\beta)$ and $\\text{Var}[W] = \\alpha \\beta / ((\\alpha + \\beta)^2 (\\alpha + \\beta + 1))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11a)\n",
    "\n",
    "We first compute the maximum likelihood estimator $\\widehat{p} = (\\widehat{p_1},...,\\widehat{p_n})$\n",
    "\n",
    "This is a multiparameter problem. As the $X_i$ are independent, the joint PMF can be written as a product of Bernoulli PMFs. \n",
    "\n",
    "Evaluating the joint PMF for a particular data set $X^n = x^n$, the likelihood function is a function of the $p_i$:\n",
    "\n",
    "$$L(p_1, ..., p_n) = \\prod^n_{i=1} p_i^{x_i} (1 - p_i)^{1- x_i}$$\n",
    "\n",
    "The log-likelihood is:\n",
    "\n",
    "$$l(p_1, ..., p_n) = \\sum^n_{i=1} x_i \\log p_i + (1- x_i) \\log(1- p_i)$$\n",
    "\n",
    "Computing partial derivatives with respect to $p_i$, setting this equal to 0, and solving for $p_i$:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial p_i} l(p_1, ..., p_n) = \\frac{x_i}{p_i} - \\frac{1-x_i}{1-p_i} = 0 \\implies \\widehat{p_i} = x_i$$\n",
    "\n",
    "Hence we have that the maximum likelihood estimator is $\\widehat{p} = (X_1, ..., X_n)$, i.e. the particular observation of the data set that is realised.\n",
    "\n",
    "Now the $\\psi = g(p_1, ..., p_n) =  \\sum^n_{i=1} p_i$ is a function of the $n$ parameters $p_1, ..., p_n$. We can compute the maximum likelihood estimator $\\widehat{\\psi}$ using the equivariance property:\n",
    "\n",
    "$$\\widehat{\\psi} = g(\\widehat{p_1}, ..., \\widehat{p_n}) = \\sum^n_{i=1} \\widehat{p_i} = \\sum^n_{i=1} X_i$$\n",
    "\n",
    "Hence the maximum likelihood estimator $\\widehat{\\psi} = n\\bar{X}_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11b)\n",
    "\n",
    "The mean squared error of the maximum likelihood estimator $\\widehat{psi}$ is the sum of the variance and squared bias of the estimator:\n",
    "\n",
    "$$\\text{MSE} = B^2 + V$$\n",
    "\n",
    "Computing the squared bias, we have***:\n",
    "\n",
    "$$B^2 = (\\mathbb{E}_{\\psi}[\\widehat{\\psi}] - \\psi)^2 = (\\mathbb{E}_{\\psi}\\left[ \\textstyle \\sum^n_{i=1} X_i\\right] - \\psi)^2 = \\left(\\textstyle \\sum^n_{i=1} \\mathbb{E}[X_i] - \\psi \\right)^2 = \\left(\\textstyle \\sum^n_{i=1} p_i - \\psi \\right)^2 = 0$$\n",
    "\n",
    "Hence this maximum likelihood estimator $\\widehat{\\psi}$ is unbiased $B = 0$.\n",
    "\n",
    "As the the estimator is unbiased, the mean squared error will be equal to the variance of the estimator, which is:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = V = \\text{Var}_{\\psi}[\\widehat{\\psi}] = \\text{Var}_{\\psi} \\left[\\textstyle \\sum^n_{i=1} X_i \\right] = \\sum^n_{i=1} \\text{Var} [X_i] + \\sum_{i \\neq j} \\text{Cov}[X_i, X_j] = \\sum^n_{i=1} \\text{Var}[X_i]  = \\sum^n_{i=1} p_i (1 - p_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11c)\n",
    "\n",
    "To compute the Bayes estimator, we assume that the parameters $p_i$ are random variables, and not fixed unknown parameters. In this context, we can now define a prior distribution on the value of the parameters $p_i$, and also a posterior distribution. \n",
    "\n",
    "We have that the posterior is:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(p_1, ..., p_n | X_1, ..., X_n) &\\propto p(X_1, ..., X_n | p_1, ..., p_n) \\pi(p_1, ..., p_n) \\\\\n",
    "&\\propto \\prod^n_{i=1} p_i^{X_i} (1- p_i)^{1-X_i} \\\\\n",
    "&= \\frac{\\prod^n_{i=1} p_i^{X_i} (1- p_i)^{1-X_i}}{\\int ... \\int \\prod^n_{i=1} p_i^{X_i} (1- p_i)^{1-X_i}dp_1 ... dp_n} \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "As the prior is uniform, and the likelihood factorises into $n$ individual likelihoods on $X_i$, it is also the case that the posterior distribution of the parameters can be factorised into $n$ inddividual posterior distributions on each of the $p_i$, which implies independence of the parameters $p_i$:\n",
    "\n",
    "$$p(p_1, ..., p_n | X_1, ..., X_n) = p(p_1 | X_1) \\cdot p(p_2 |X_2) \\cdot \\space  ...  \\space \\cdot p(p_n |X_n)$$\n",
    "\n",
    "Where each individual posterior has the form:\n",
    "\n",
    "$$p(p_i | X_i) = p_i^{X_i} (1 - p_i)^{1-X_i}$$\n",
    "\n",
    "This posterior can be written as a Beta distribution up to a constant of proportionality, that is $p_i \\sim \\text{Beta}(X_i + 1, -X_i + 2)$, that is:\n",
    "\n",
    "$$p(p_i | X_i) \\propto \\frac{\\Gamma(3)}{\\Gamma(X_i + 1) \\Gamma(-X_i + 2)} p_i^{X_i}(1 - p_i)^{1-X_i}$$\n",
    "\n",
    "And hence $p_i$ has posterior mean:\n",
    "\n",
    "$$\\mathbb{E}[p_i | X_i] = \\frac{X_i + 1}{(X_i + 1) + (-X_i + 2)} = \\frac{X_i + 1}{3}$$\n",
    "\n",
    "Using the above results, and recalling that $\\psi = g(p_1, ... , p_n)$,  we can compute the Bayes estimator $\\widehat{\\psi}$ in the following manner:\n",
    "\n",
    "$$\\widehat{\\psi} = \\mathbb{E}[\\psi | X_1... X_n] = \\mathbb{E} \\left[\\textstyle \\sum^n_{i=1} p_i | X_1 , ..., X_n \\right] = \\sum^n_{i=1} \\mathbb{E}[p_i | X_1, ... , X_n] = \\sum^n_{i=1} \\mathbb{E}[p_i | X_i] = \\sum^n_{i=1} \\frac{X_i + 1}{3} = \\frac{n}{3}(\\bar{X}_n + 1)$$\n",
    "\n",
    "Where we have used the law of the unconscious statistician, linearity of expectation; and the fact that the $p_i$ are independent in going from the 3rd to the 4th equality. This needs further justification in terms of what distributions expectations are being taken with respect to, so we show the derivation using integrals:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}[\\psi | X_1 ... X_n] &= \\int ... \\int \\left(\\sum^n_{i=1} p_i \\right) p(p_1, ..., p_n | X_1, ..., X_n) dp_1 ... dp_n \\\\\n",
    "&= \\int ... \\int p_1 p(p_1, ..., p_n | X_1, ..., X_n) dp_1 ... dp_n + \\space ... \\space + \\int ... \\int p_n p(p_1, ..., p_n | X_1, ..., X_n) dp_1 ... dp_n \\\\\n",
    "&= \\sum^n_{i=1}\\mathbb{E}[p_i | X_1 ... X_n]\n",
    "\\end{align}$$\n",
    "\n",
    "Where we have the used the fact that definite integration is distributive. Denoting $X_{-i}$ and $p_{-i}$ to refer to all the $X_k$ except for $X_i$, and all the $p_k$ except $p_i$ respectively, each summand can be simplified as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}[p_i | X_1 ... X_n] &= \\int ... \\int p_i p(p_1, ..., p_N | X_1, ..., X_n) dp_1 ... dp_n \\\\\n",
    "&= \\int ... \\int p(p_{-i} | X_{-i}) \\int p_i p(p_i | X_i) dp_i dp_{-i} \\\\\n",
    "&= \\int ... \\int p(p_{-i} | X_{-i}) \\int^{1}_{0} p_i p(p_i | X_i) dp_i dp_{-i} \\\\ \n",
    "&= \\int ... \\int p(p_{-i} | X_{-i}) \\mathbb{E}[p_i | X_i] dp_{-i} \\\\\n",
    "&= \\mathbb{E}[p_i | X_i] \\underbrace{\\int ... \\int p(p_{-i} | X_{-i}) dp_{-i}}_{=1} \\\\\n",
    "&= \\mathbb{E}[p_i | X_i] \n",
    "\\end{align}$$\n",
    "\n",
    "Yielding the required result. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Let $X_1, ... , X_n \\sim N(\\mu, 1)$.\n",
    "\n",
    "a) Let $T = \\text{max}\\{X_1, ..., X_n \\}$. Show that $T$ is not sufficient.\n",
    "\n",
    "b) Let use the improper prior $\\pi(\\mu) \\propto 1$. Find the Bayes estimator of $\\psi = \\mu^2$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
