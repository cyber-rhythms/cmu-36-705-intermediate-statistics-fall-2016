{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 36-705 Intermediate Statistics.\n",
    "\n",
    "### Homework 4.\n",
    "\n",
    "### INSERT DATE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Let $X_1, ..., X_n \\sim N(\\mu, \\Sigma)$ where $X_i \\in \\mathbb{R}^d$, $\\mu \\in \\mathbb{R}^d$, and $\\Sigma \\in \\mathbb{R}^{d x d}$.\n",
    "\n",
    "a) Find a minimal sufficient statistic.\n",
    "\n",
    "b) Show that $X_1 + X_2$ is not a sufficient statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose the following statistic:\n",
    "\n",
    "$$T(x^n) = \\left( \\bar{X}_n, S^2 \\right) = \\left(\\frac{1}{n} \\sum^n_{i=1} X_i \\space , \\space \\frac{1}{n} \\sum^n_{i=1} (X_i - \\bar{X}_n)(X_i - \\bar{X}_n)^T \\right)$$\n",
    "\n",
    "Where $\\bar{X}_n \\in \\mathbb{R}^d$ is the sample mean and $S^2 \\in \\mathbb{R}^{d x d}$ is the sample variance.\n",
    "\n",
    "We will check that this is sufficient, then whether it is minimal sufficient.\n",
    "\n",
    "The Fisher-Neyman factorisation theorem states that the statistic $T(x^n)$ is sufficient for the parameter $\\boldsymbol{\\theta} = \\{ \\mu, \\Sigma  \\}$ iff the joint pdf $(x^n ; \\boldsymbol{\\theta})$  can be factored like so:\n",
    "\n",
    "$$p(x^n ; \\boldsymbol{\\theta}) = h(x^n) g(t; \\boldsymbol{\\theta})$$\n",
    "\n",
    "Where $h(x_1,...,x_n)$ is a function that does not depend on the parameter $\\boldsymbol{\\theta}$, and where $g(t(x_1,...,x_n); \\boldsymbol{\\theta})$ is a function depends on the parameter $\\boldsymbol{\\theta}$, and indirectly on the data $x_1,...,x_n$ through the statistic $t(x_1,...,x_n)$.\n",
    "\n",
    "The data has the following joint PDF:\n",
    "\n",
    "$$\\begin{align}\n",
    "p(X_1,..., X_n; \\boldsymbol{\\theta}) &= \\frac{1}{(2 \\pi)^{\\frac{nd}{2}} | \\Sigma |^{\\frac{d}{2}}} \\exp \\left\\{-\\frac{1}{2} \\sum^n_{i=1} (X_i - \\mu)^T \\Sigma^{-1} (X_i - \\mu) \\right\\} \\\\\n",
    "&= \\frac{1}{(2 \\pi)^{\\frac{nd}{2}} | \\Sigma |^{\\frac{d}{2}}} \\exp \\left\\{-\\frac{1}{2} \\sum^n_{i=1} \\left[ (X_i - \\bar{X}_n)^T \\Sigma^{-1} (X_i - \\bar{X}_n) + (\\bar{X}_n - \\mu)^T \\Sigma^{-1} (\\bar{X}_n - \\mu) \\right] \\right\\} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where we have used relied the \"trick\" of expressing $\\sum^n_{i=1} (X_i - \\mu)^T \\Sigma^{-1} (X_i - \\mu)$ in terms of an additional sample mean $\\bar{X}_n$. Now as both terms within the summation are of the form $\\mathbf{x}^T \\mathbf{A} \\mathbf{x}$ where $\\mathbf{x} \\in \\mathbb{R}^d$ and $\\mathbf{A} \\in \\mathbb{R}^{dxd}$, they are quadratic forms and hence scalars.\n",
    "\n",
    "Using the property that the trace of scalar is a scalar $\\text{tr}(a) = a$, the invariance of the operator to cyclic permutations $\\text{tr}(ABC) = \\text{tr}(CAB) = \\text{tr}(BCA)$, and the fact that the last quadratic form within the summation is not indexed by the summation, we have that:\n",
    "\n",
    "\n",
    "$$\\begin{align}\n",
    "p(X_1,..., X_n; \\boldsymbol{\\theta}) &= \\frac{1}{(2 \\pi)^{\\frac{nd}{2}} | \\Sigma |^{\\frac{d}{2}}} \\exp \\left\\{-\\frac{1}{2} \\sum^n_{i=1} \\text{tr} \\left[ (X_i - \\bar{X}_n)^T \\Sigma^{-1} (X_i - \\bar{X}_n) \\right] \\right\\} \\exp \\left\\{-\\frac{n}{2}(\\bar{X}_n - \\mu)^T \\Sigma^{-1} (\\bar{X}_n - \\mu) \\right\\} \\\\\n",
    "&= \\frac{1}{(2 \\pi)^{\\frac{nd}{2}} | \\Sigma |^{\\frac{d}{2}}} \\exp \\left\\{-\\frac{1}{2} \\sum^n_{i=1} \\text{tr} \\left[ (X_i - \\bar{X}_n)^T \\Sigma^{-1} (X_i - \\bar{X}_n) \\right] \\right\\} \\exp \\left\\{-\\frac{n}{2}(\\bar{X}_n - \\mu)^T \\Sigma^{-1} (\\bar{X}_n - \\mu) \\right\\} \\\\\n",
    "&= \\frac{1}{(2 \\pi)^{\\frac{nd}{2}} | \\Sigma |^{\\frac{d}{2}}} \\exp \\left\\{-\\frac{1}{2} \\sum^n_{i=1} \\text{tr} \\left[  \\Sigma^{-1} (X_i - \\bar{X}_n) (X_i - \\bar{X}_n)^T \\right] \\right\\} \\exp \\left\\{-\\frac{n}{2}(\\bar{X}_n - \\mu)^T \\Sigma^{-1} (\\bar{X}_n - \\mu) \\right\\} \\\\\n",
    "&= \\frac{1}{(2 \\pi)^{\\frac{nd}{2}} | \\Sigma |^{\\frac{d}{2}}} \\exp \\left\\{-\\frac{1}{2} \\text{tr} \\sum^n_{i=1}  \\left[  \\Sigma^{-1} (X_i - \\bar{X}_n) (X_i - \\bar{X}_n)^T \\right] \\right\\} \\exp \\left\\{-\\frac{n}{2}(\\bar{X}_n - \\mu)^T \\Sigma^{-1} (\\bar{X}_n - \\mu) \\right\\} \\\\\n",
    "&= \\frac{1}{(2 \\pi)^{\\frac{nd}{2}} | \\Sigma |^{\\frac{d}{2}}} \\exp \\left\\{-\\frac{n}{2} \\text{tr} \\left[  \\Sigma^{-1} \\frac{1}{n}\\sum^n_{i=1}   (X_i - \\bar{X}_n) (X_i - \\bar{X}_n)^T \\right] \\right\\} \\exp \\left\\{-\\frac{n}{2}(\\bar{X}_n - \\mu)^T \\Sigma^{-1} (\\bar{X}_n - \\mu) \\right\\} \\\\\n",
    "\\implies p(X_1,..., X_n; \\boldsymbol{\\theta}) &= \\frac{1}{(2 \\pi)^{\\frac{nd}{2}} | \\Sigma |^{\\frac{d}{2}}} \\exp \\left \\{-\\frac{n}{2} \\left[\\text{tr}(\\Sigma^{-1} S^2) +  (\\bar{X}_n - \\mu)^T \\Sigma^{-1} (\\bar{X}_n - \\mu) \\right]  \\right \\}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Setting the RHS as $g(\\bar{X}_n, S^2; \\mu, \\Sigma)$ and $h(x_1,...,x_n) = 1$, we have that $T = (\\bar{X}_n, S^2)$ is a sufficient statistic for $\\boldsymbol{\\theta}$.\n",
    "\n",
    "Now $T$ is minimal sufficient iff it has the property that $R(x^n, y^n; \\boldsymbol{\\theta})$ does not depend on $\\boldsymbol{\\theta}$ if and only if $T(x^n) = T(y^n)$, where:\n",
    "\n",
    "$$R(x^n, y^n; \\boldsymbol{\\theta}) = \\frac{p(x^n; \\boldsymbol{\\theta})}{p(y^n; \\boldsymbol{\\theta})}$$\n",
    "\n",
    "Showing that $T(x^n) = T(y^n) \\implies R(x^n, y^n; \\boldsymbol{\\theta})$ does not depend on $\\boldsymbol{\\theta}$:\n",
    "\n",
    "Consider $R(x^n, y^n; \\boldsymbol{\\theta})$:\n",
    "\n",
    "$$R(x^n, y^n; \\boldsymbol{\\theta}) = \\frac{\\frac{1}{(2 \\pi)^{\\frac{nd}{2}} | \\Sigma |^{\\frac{d}{2}}} \\exp \\left \\{-\\frac{n}{2} \\left[\\text{tr}(\\Sigma^{-1} S_{x^n}^2) +  (\\bar{X}_n - \\mu)^T \\Sigma^{-1} (\\bar{X}_n - \\mu) \\right]  \\right \\}}{\\frac{1}{(2 \\pi)^{\\frac{nd}{2}} | \\Sigma |^{\\frac{d}{2}}} \\exp \\left \\{-\\frac{n}{2} \\left[\\text{tr}(\\Sigma^{-1} S_{y^n}^2) +  (\\bar{Y}_n - \\mu)^T \\Sigma^{-1} (\\bar{Y}_n - \\mu) \\right]  \\right \\}} = \\frac{\\text{tr}(\\Sigma^{-1} S_{x^n}^2) +  (\\bar{X}_n - \\mu)^T \\Sigma^{-1} (\\bar{X}_n - \\mu)}{\\text{tr}(\\Sigma^{-1} S_{y^n}^2) +  (\\bar{Y}_n - \\mu)^T \\Sigma^{-1} (\\bar{Y}_n - \\mu)}$$\n",
    "\n",
    "If $T(x^n) = (\\bar{X}_n, S^2_{x^n}) = (\\bar{Y}_n, S^2_{y^n}) = T(y^n)$, then we have that $R(x^n, y^n; \\boldsymbol{\\theta}) = 1$, which does not depend on $\\boldsymbol{\\theta}$.\n",
    "\n",
    "Showing that $ R(x^n, y^n; \\boldsymbol{\\theta})$ does not depend on $\\boldsymbol{\\theta} \\implies T(x^n) = T(y^n)$:\n",
    "\n",
    "Noting that the parameters are fixed and unknown, the only case where there \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Let $X_1, X_2 \\sim \\text{Uniform}(0, \\theta)$ where $\\theta > 0$.\n",
    "\n",
    "a) Find the distribution of $(X_1, X_2)$ given $T$ where $T = \\text{max}\\{X_1, X_2 \\}$.\n",
    "\n",
    "b) Show that $X_1 + X_2$ is not sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a)\n",
    "\n",
    "One way of assessing the sufficiency of a statistic $T$ is by computing the conditional distribution $f_{X_1, X_2 | T} (x_1, x_2 | t ; \\theta)$. If it does not depend on the parameter $\\theta$, that is, if we can show that it is of the form, $f_{X_1, X_2 | T} (x_1, x_2 | t)$, where the $\\theta$ does not appear, then $T$ is sufficient.\n",
    "\n",
    "Intuitively, after conditioning on a sufficient statistic, the conditional distribution of the data does not change according to the setting of the parameter. \n",
    "\n",
    "The conditional PDF $f_{X_1, X_2 | T} (x_1, x_2 | t; \\theta)$ in terms of joint and marginal PDFs is given by:\n",
    "\n",
    "$$f_{X_1, X_2 | T} (x_1, x_2 | t; \\theta) = \\frac{f_{X_1, X_2, T} (x_1, x_2, t; \\theta)}{f_{t}(t; \\theta)} = \\frac{f_{X_1, X_2} (x_1, x_2 ; \\theta)}{f_{T}(t; \\theta)}$$\n",
    "\n",
    "**Insert reason**\n",
    "\n",
    "Assuming $X_1$ and $X_2$ are independent, they have the following joint density function:\n",
    "\n",
    "$$f_{X_1, X_2}(x_1, x_2, ;\\theta) =\n",
    "\\begin{cases}\n",
    "\\left(\\frac{1}{\\theta}\\right)^2 \\quad &0 \\leq x_1 \\leq \\theta , 0 \\leq x_2 \\leq \\theta \\\\\n",
    "0 &\\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "To compute the PDF of our statistic $T = \\text{max} \\{ x_1 , x_2 \\}$, which is a transformation of the random variables $X_1, X_2$, we find the CDF $F_{T}(t)$ and differentiate to find the PDF $f_T({t})$:\n",
    "\n",
    "$$F_{T}(t) = P(T \\leq t) = P(\\left\\{\\text{max} \\{x_1 , x_2 \\} \\leq t\\right\\} ) = \\int \\int_{A_{t}} f_{X_1, X_2}(x_1, x_2 ; \\theta) dx_1 dx_2$$\n",
    "\n",
    "We now need to find the set $A_t = \\{(x_1, x_2) : \\text{max}\\{x_1, x_2\\} \\leq t  \\}$ for relevant values of $t$. Noting that $X_1, X_2 \\sim \\text{Uniform}(0, \\theta)$; the minimum value of $t = 0$ occurs when either $X_1 = 0$ or $X_2 = 0$ or both, and applying similar arguments to the maximum value of $t$, we have that the relevant interval is $0 \\leq t \\leq \\theta$.\n",
    "\n",
    "We can visualise the set of points contained by contour plots, with each iso-contour corresponding to the set where $t = \\text{max} \\{x_1, x_2 \\}$ and find an expression for this area in terms of $t$.\n",
    "\n",
    "We find that the set $A_t$ corresponds to a shaded square of length $t$, area $t^2$ inside a square of area $\\theta^2$. This gives the following CDF $F_T(t)$:\n",
    "\n",
    "$$F_T(t) = \n",
    "\\begin{cases}\n",
    "0 &t < 0 \\\\\n",
    "\\left(\\frac{t}{\\theta} \\right)^2 & 0 \\leq t \\leq \\theta \\\\\n",
    "1 &t > 0 \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Differentiating yields the following PDF:\n",
    "\n",
    "$$f_T(t) = \n",
    "\\begin{cases}\n",
    "\\left(\\frac{2t}{\\theta^2} \\right) &0 \\leq t \\leq \\theta \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Hence the conditional distribution of $X_1, X_2$ given $T = \\text{max}\\{x_1, x_2 \\}$ is:\n",
    "\n",
    "$$f_{X_1, X_2 | T; \\theta}(x_1, x_2 | t; \\theta) = \\frac{\\frac{1}{\\theta^2} \\cdot \\mathbb{I}(0 \\leq x_1, x_2 \\leq \\theta)}{\\frac{2t}{\\theta^2} \\cdot \\mathbb{I}(0 \\leq t \\leq \\theta)} = \\frac{1 \\cdot \\mathbb{I}(x_{(2)} \\leq \\theta)}{2t \\cdot \\mathbb{I}(t \\leq \\theta) } = \\frac{1}{2t}$$\n",
    "\n",
    "Where we have used dropped the lower bound of 0 in the indicator functions as they are redundant - both $X_1, X_2$ are greater than 0 by assumption, meaning that $T$ is also greater than 0. And where we have  because $X_1, X_2 \\leq \\theta \\implies \\text{max} \\{x_1, x_2 \\} \\leq \\theta \\implies t \\leq \\theta$.\n",
    "\n",
    "The conditional distribution does not depend on $\\theta$ and hence $T$ is sufficient.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b)\n",
    "\n",
    "Following the same arguments as above, we compute conditional distribution of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Let $X_1, X_2, ... , X_n \\sim \\text{Uniform}(-\\theta, 2 \\theta)$ where $\\theta > 0$. Find the likelihood function. \n",
    "\n",
    "As the data is IID, denoting the PMF of $X_i$ parametrised by $\\theta$ as $f_{X_i} (x_i ; \\theta)$, the likelihood function $L(\\theta)$ is given by:\n",
    "\n",
    "$$L(\\theta) = \\prod^n_{i=1} f_{X_i} (x_i ; \\theta)$$\n",
    "\n",
    "The $L(\\theta) =0$ if any of the PMFs $f_{X_i} (X_i ; \\theta) = 0$.\n",
    "\n",
    "That means we only need to consider the 1st and $n$th order statistics of the data, that is $X_{(1)} = \\text{min} \\{X_1,..., X_n \\}$ and $X_{(n)} = \\text{max} \\{X_1,..., X_n\\}$, and their values relative to the parameters. \n",
    "\n",
    "We can define the likelihood function piecewise like so:\n",
    "\n",
    "$$L(\\theta) = \\begin{cases}\n",
    "0 \\quad \\text{if} \\quad -\\theta > X_{(1)} \\\\\n",
    "0 \\quad \\text{if} \\quad 2 \\theta < X_{(n)} \\\\\n",
    "0 \\quad \\text{if} \\quad 2\\theta < X_{(n)} \\text{and} \\space -\\theta > X_{(1)} \\\\\n",
    "\\left(\\frac{1}{3 \\theta} \\right)^n \\quad \\text{if} \\quad 2\\theta > X_{(n)} \\text{and} \\space -\\theta < X_{(1)} \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Hence the likelihood function is given by:\n",
    "\n",
    "$$L(\\theta) = \\mathbb{I}(2 \\theta > X_{(n)}) \\mathbb{I}(- \\theta < X_{(1)}) \\left( \\frac{1}{3 \\theta} \\right)^n$$\n",
    "\n",
    "**[INSERT PICTURE]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Chapter 6 problem 1.\n",
    "\n",
    "Let $X_1,..., X_n \\sim \\text{Poisson}(\\lambda)$, and let $\\widehat{\\lambda} = \\frac{1}{n} \\sum^n_{i=1} X_i$.\n",
    "\n",
    "Find the bias, standard error, and mean squared error of this estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have that the mean squared error is the sum of the squared bias and variance of an estimator:\n",
    "\n",
    "$$\\text{MSE} = B^2 + V$$\n",
    "\n",
    "$$\\mathbb{E}[(\\lambda - \\widehat{\\lambda})^2] = \\left(\\mathbb{E}[\\widehat{\\lambda}] - \\lambda \\right)^2 + \\mathbb{E}\\left[(\\widehat{\\lambda} - \\mathbb{E}[\\widehat{\\lambda}])^2 \\right]$$\n",
    "\n",
    "where the expectation, $\\mathbb{E}[\\cdot]$, is with respect to a joint probability distribution parameterised by a value of the parameter at its true value, $\\lambda$.\n",
    "\n",
    "As $\\widehat{\\lambda}$, being a sample mean, is an unbiased estimator of $\\lambda$, we have that the bias is equal to 0, $B = 0$, and the mean squared error is equal to the variance, $\\text{MSE} = V$.\n",
    "\n",
    "As $\\widehat{\\lambda}$ is a sample mean, and the data is IID, the mean and variance of $\\widehat{\\lambda}$ are given by:\n",
    "\n",
    "$$\\mathbb{E}[\\widehat{\\lambda}] = \\lambda, \\quad \\text{Var}[\\widehat{\\lambda}] = \\frac{\\lambda}{n}$$\n",
    "\n",
    "Hence $\\text{MSE} = V = \\text{Var}[\\widehat{\\lambda}] = \\frac{\\lambda}{n}$.\n",
    "\n",
    "And the standard error is given by $\\text{se} = {\\sigma}_{\\widehat{\\lambda}} = \\sqrt{\\frac{\\lambda}{n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Chapter 6, problem 3.\n",
    "\n",
    "Let $X_1, ..., X_n \\sim \\text{Uniform}(0, \\theta)$, and let $\\widehat{\\theta} = 2 \\bar{X}_n$.\n",
    "\n",
    "Find the bias, standard error, and mean squared error of this estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IID random variables with uniformly distributed $X_i \\sim \\text{Unif}(a, b)$ have mean and variance given by:\n",
    "\n",
    "$$\\mathbb{E}[X_i] = \\frac{a + b}{2} = \\frac{\\theta}{2}$$\n",
    "\n",
    "$$\\text{Var}[X_i] = \\frac{(b-a)^2}{12} = \\frac{\\theta^2}{12}$$\n",
    "\n",
    "Using the same formulae for mean squared error as above, we have that:\n",
    "\n",
    "\n",
    "$$\\text{MSE} = B^2 + V$$\n",
    "\n",
    "$$\\text{MSE} = \\mathbb{E}_{\\theta}[(\\theta - \\widehat{\\theta})^2] = \\left(\\mathbb{E}_{\\theta}[\\widehat{\\theta}] - \\theta \\right)^2 + \\mathbb{E}_{\\theta}\\left[(\\widehat{\\theta} - \\mathbb{E}_{\\theta}[\\widehat{\\theta}])^2 \\right]$$\n",
    "\n",
    "Computing the mean of the estimator, we have that:\n",
    "\n",
    "$$\\mathbb{E}_{\\theta}[\\widehat{\\theta}] = \\mathbb{E}[2 \\bar{X}_n] = 2 \\mathbb{E}[\\bar{X}_n] = 2 \\mathbb{E}[X_i] = 2 \\left(\\frac{\\theta}{2} \\right) = \\theta$$\n",
    "\n",
    "Hence we have that the estimator $\\widehat{\\theta}$ is unbiased, and that $ B = 0$.\n",
    "\n",
    "The variance of the estimator, in terms of its 2nd moment and mean is given by:\n",
    "\n",
    "$$\\begin{align}\\text{Var}[\\widehat{\\theta}] &= \\mathbb{E}[\\widehat{\\theta}^2] - \\mathbb{E}[\\widehat{\\theta}]^2 \\\\\n",
    "&= \\mathbb{E}[(2 \\bar{X}_n)^2] - \\mathbb{E}[2 \\bar{X}_n]^2 \\\\\n",
    "&= 4 \\mathbb{E}[\\bar{X}^2_n] - \\theta^2 \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Computing the 2nd moment of the sample mean yields:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}[\\bar{X}^2_n] &= \\text{Var}[\\bar{X}_n] + \\mathbb{E}[\\bar{X}_n]^2 \\\\\n",
    "&= \\frac{1}{n} \\text{Var}[X_i] + \\theta^2 \\\\\n",
    "&= \\frac{\\theta^2}{12n} + \\theta^2 \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "We can now compute the variance of the estimator:\n",
    "\n",
    "$$\\text{Var}[\\widehat{\\theta}] = 4 \\left( \\frac{\\theta^2}{12n} + \\theta^2 \\right) - \\theta^2 = \\frac{\\theta^2}{3n} + 3 \\theta^2$$\n",
    "\n",
    "The standard error is given by:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{se} = \\sigma_{\\hat{\\theta}} = \\sqrt{\\text{Var}_{\\theta}(\\widehat{\\theta})} &= \\sqrt{\\left( \\frac{\\theta^2}{3n} + 3 \\theta^2 \\right)} \\\\\n",
    "&= \\sqrt{\\frac{9n \\theta^2 + \\theta^2}{3n}} \\\\\n",
    "&= \\sqrt{\\frac{\\theta^2 (9n + 1)}{3n}} \\\\\n",
    "&= \\theta \\sqrt{\\frac{9n+1}{3n}} \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Hence we have that the estimator $\\widehat{\\theta} = 2 \\bar{X}_n$ is an unbiased of $\\theta$, i.e.  $B = 0$.\n",
    "\n",
    "As the estimator is unbiased, the mean squared error is equal to the variance $\\text{MSE} = V  = \\frac{\\theta^2}{3n} + 3\\theta^2$.\n",
    "\n",
    "And the standard error is given by $\\text{se} = \\sigma_{\\hat{\\theta}} = \\theta \\sqrt{(9n+1) / 3n)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Chapter 9, problem 2 (a,b,c).\n",
    "\n",
    "Let $X_1,..., X_n \\sim \\text{Unif}(a, b)$ where $a$ and $b$ are unknown paramters with $a < b$.\n",
    "\n",
    "a) Find the method of moments estimator for $a$ and $b$.\n",
    "\n",
    "b) Find the maximum likelihood estimator, $\\widehat{a}$ and $\\widehat{b}$.\n",
    "\n",
    "c) Let $\\tau = \\int x d F(x)$, and find the maximum likelihood estimator of $\\tau$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6a) The mean and variance of a random variable $X_i \\sim N(a,b)$ where $a < b$ is given by:\n",
    "\n",
    "$$\\mathbb{E}[X_i] = \\frac{a + b}{2}$$\n",
    "\n",
    "$$\\text{Var}[X_i] = \\frac{(b-a)^2}{12}$$\n",
    "\n",
    "To find the method of moments estimators $\\widehat{a}_{MoM}$ and $\\widehat{b}_{MoM}$, we equate $k$th sample moment $m_k = \\frac{1}{n} \\sum^n_{i=1} X^k_i$ with the $k$th theoretical moment $\\mathbb{E}[X^k_i]$ for $k = 1, 2$; i.e. the means and variances:\n",
    "\n",
    "Equating the sample mean and population mean (1st moments):\n",
    "\n",
    "$$m_1 = \\mathbb{E}[X_i] \\implies \\frac{1}{n} \\sum^n_{i=1} X_i = \\frac{a+b}{2}$$\n",
    "\n",
    "Computing the 2nd theoretical moment:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}[X^2_i] &= \\text{Var}[X_i] + \\mathbb{E}[X_i]^2 \\\\\n",
    "&= \\frac{(b-a)^2}{12} + \\left( \\frac{a+b}{2} \\right)^2 \\\\\n",
    "&= \\frac{(b-a)^2}{12} + \\frac{3(a+b)^2}{12} \\\\\n",
    "&= \\frac{3(a^2 + 2ab + b^2) + (b^2 - 2ab + a^2)}{12} \\\\\n",
    "&= \\frac{4a^2 + 4ab + 4b^2}{12}\\\\\n",
    "&= \\frac{a^2 + ab + b^2}{3} \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Equating the 2nd sample and theoretical moment, we have that:\n",
    "\n",
    "$$m_2 = \\mathbb{E}[X_i^2] \\implies \\frac{1}{n} \\sum^n_{i=1} X^2_i = \\frac{a^2 + ab + b^2}{3}$$\n",
    "\n",
    "Setting $b = 2\\bar{X}_n - a$, and substituting into the 2nd moment equation one line above, we have that:\n",
    "\n",
    "$$\\begin{align}\n",
    "3 \\left( \\frac{1}{n} \\sum^n_{i=1} X_i^2 \\right) &= a^2 + a(2 \\bar{X}_n - a) + (2 \\bar{X}_n - a)^2  \\\\\n",
    "&= a^2 + 2a \\bar{X}_n - a^2 + 4 \\bar{X}_n^2 - 4a \\bar{X}_n + a^2 \\\\\n",
    "&= a^2 - 2a \\bar{X}_n + 4 \\bar{X}_n^2 \\\\\n",
    "&= (a - \\bar{X}_n)^2 + 3 \\bar{X}_n^2 \\\\\n",
    "\\implies \\widehat{a}_{MoM} &= \\bar{X}_n + \\sqrt{3 \\left( \\frac{1}{n} \\sum^n_{i=1} X^2_i \\right) - 3 \\left( \\bar{X}_n \\right)^2} \\\\\n",
    "&= m_1 + \\sqrt{3(m_2 - m^2_1)}\n",
    "\\end{align}$$\n",
    "\n",
    "And we also have that:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\widehat{b}_{MoM} &= 2 \\bar{X}_n - \\widehat{a}_{MoM} \\\\\n",
    "&= \\bar{X}_n - \\sqrt{3 \\left( \\frac{1}{n} \\sum^n_{i=1} X^2_i \\right) - 3 \\left( \\bar{X}_n \\right)^2} \\\\\n",
    "&= m_1 - \\sqrt{3(m_2 - m^2_1)}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6b)\n",
    "\n",
    "The PMF of $X_i \\sim \\text{Unif}(a, b)$ is given by:\n",
    "\n",
    "$$f_{X_i}(x_i ; a, b) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{b-a} \\quad &\\text{if} \\quad x \\in [a, b] \\\\\n",
    "0 \\quad &\\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "We first need to derive the likelihood function. Much of the reasoning for deriving the likelihood function under a uniform distribution is similar to question 3) above, so we lift from there.\n",
    "\n",
    "As the data is IID, denoting the PMF of $X_i$ parametrised by $a, b$ as $f_{X_i} (x_i ; a, b)$, the likelihood function $L(a, b)$ is given by:\n",
    "\n",
    "$$L(a, b) = \\prod^n_{i=1} f_{X_i} (x_i ; a, b)$$\n",
    "\n",
    "Then $L(a, b) =0$ if any of the PMFs $f_{X_i} (X_i ; a, b) = 0$.\n",
    "\n",
    "That means we only need to consider the 1st and $n$th order statistics of the data, that is $X_{(1)} = \\text{min} \\{X_1,..., X_n \\}$ and $X_{(n)} = \\text{max} \\{X_1,..., X_n\\}$, and their values relative to the parameters. \n",
    "\n",
    "$$L(a, b) = \\begin{cases}\n",
    "0 \\quad &\\text{if} \\quad a > X_{(1)} \\\\\n",
    "0 \\quad &\\text{if} \\quad b < X_{(n)} \\\\\n",
    "0 \\quad &\\text{if} \\quad b < X_{(n)} \\space \\text{and} \\space a > X_{(1)} \\\\\n",
    "\\left(\\frac{1}{b-a} \\right)^n \\quad &\\text{if} \\quad b \\geq X_{(n)} \\space \\text{and} \\space a \\leq X_{(1)} \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Hence the likelihood function is given by:\n",
    "\n",
    "$$L(a, b) = \\mathbb{I}(b \\geq X_{(n)}) \\mathbb{I}(a \\leq X_{(1)})\\left( \\frac{1}{b-a} \\right)^n$$\n",
    "\n",
    "The maximum likelihood estimators $\\widehat{a}_{\\text{ML}}$ and $\\widehat{b}_{\\text{ML}}$ are set so as to maximise the likelihood function $L(a, b)$.\n",
    "\n",
    "In order to maximise $L(a,b)$, we need to minimise $(b-a)$ in such a way that $L(a,b) \\neq 0$, i.e. with constraints that $a \\leq X_{(1)}$ and  $b \\geq X_{(n)}$.\n",
    "\n",
    "The smallest we can make $(b-a)$ with the above constraints satisfied is by setting:\n",
    "\n",
    "$$\\widehat{a}_{\\text{MLE}} = X_{(1)}$$\n",
    "\n",
    "$$\\widehat{b}_{\\text{MLE}} = X_{(n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6c)\n",
    "\n",
    "$\\tau$, which is the population mean, is expressed as a function of the parameters $a, b$ of the Uniform distribution:\n",
    "\n",
    "$$\\tau = \\mathbb{E}[X_i] = \\frac{a + b}{2} = g(a, b)$$\n",
    "\n",
    "As the maximum likelihood estimator is equivariant, we have that $\\widehat{\\tau}_{\\text{MLE}} = g(\\widehat{a}_{\\text{MLE}}, \\widehat{b}_{\\text{MLE}})$.\n",
    "\n",
    "Hence we have that the maximum likelihood estimator of $\\tau$ is given by:\n",
    "\n",
    "$$\\widehat{\\tau}_{\\text{MLE}} = \\frac{\\widehat{a}_{\\text{MLE}} + \\widehat{b}_{\\text{MLE}}}{2} = \\frac{X_{(1)} + X_{(n)}}{2}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
