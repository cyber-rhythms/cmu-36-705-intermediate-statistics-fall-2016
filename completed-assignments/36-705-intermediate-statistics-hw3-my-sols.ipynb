{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 36-705 Intermediate Statistics.\n",
    "\n",
    "### Homework 3.\n",
    "\n",
    "### INSERT DATE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Let $\\mathcal{C} = \\mathcal{A} \\bigcup \\mathcal{B}$. Show that\n",
    "\n",
    "$$s_n(\\mathcal{C}) \\leq s_n(\\mathcal{A}) + s_n(\\mathcal{B})$$\n",
    "\n",
    "where $s_n$ denotes the shattering number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Let $\\mathcal{C} = \\{A \\cup B; A \\in \\mathcal{A}, B \\in \\mathcal{B} \\}$. Show that:\n",
    "\n",
    "$$s_n(\\mathcal{C}) \\leq s_n(\\mathcal{A})s_n(\\mathcal{B})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Chapter 5, problem 2.\n",
    "\n",
    "Let $X_1, X_2,...$ be a sequence of random variables. Show that $X_n \\overset{\\text{qm}}{\\rightarrow} b$ if and only if:\n",
    "\n",
    "$$\\lim_{n \\rightarrow \\infty} \\mathbb{E}[X_n] = b \\quad \\text{and} \\quad\\lim_{n \\rightarrow \\infty} \\text{Var}[X_n] = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoting proposition (I) as:\n",
    "$$X_n \\overset{\\text{qm}}{\\rightarrow} b$$\n",
    "\n",
    "And proposition (II) as:\n",
    "\n",
    "$$\\lim_{n \\rightarrow \\infty} \\mathbb{E}[X_n] = b \\quad \\text{and} \\quad\\lim_{n \\rightarrow \\infty} \\text{Var}[X_n] = 0$$\n",
    "\n",
    "\n",
    "To show that (I) $\\implies$ (II):\n",
    "\n",
    "In order for convergence in quadratic mean, $X_n \\overset{\\text{qm}}{\\rightarrow} b$, by definition, we have that:\n",
    "\n",
    "$$\\begin{align}\n",
    "&\\lim_{n \\rightarrow \\infty} \\mathbb{E}[(X_n - b)^2] = 0 \\\\\n",
    "\\implies &\\lim_{n \\rightarrow \\infty} \\mathbb{E}[(X_n^2 - 2bX_n + b^2] = 0 \\\\\n",
    "\\implies &\\lim_{n \\rightarrow \\infty} \\mathbb{E}[X_n^2] - 2b\\mathbb{E}[X_n] + b^2 = 0 \\\\\n",
    "\\implies &\\lim_{n \\rightarrow \\infty} \\text{Var}[X_n] + \\mathbb{E}[X_n]^2 - 2b\\mathbb{E}[X_n] + b^2 = 0 \\\\\n",
    "\\implies &\\lim_{n \\rightarrow \\infty} \\text{Var}[X_n] + \\lim_{n \\rightarrow \\infty} (\\mathbb{E}[X_n] - b)^2 = 0 \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Where we have substituted the 2nd moment for the sum of the variance and squared mean to get from the 3rd to the 4th equality. \n",
    "\n",
    "A property of both the variance $\\text{Var}[X_n]$ and the term $(\\mathbb{E}[X_n] - b)^2$ is that they are both non-negative. And because the RHS is 0, in order for equality to hold, we must have that both limits on the LHS be equal to 0, which occurs when:\n",
    "\n",
    "$$\\lim_{n \\rightarrow \\infty} \\mathbb{E}[X_n] = b \\quad \\text{and} \\quad\\lim_{n \\rightarrow \\infty} \\text{Var}[X_n] = 0$$\n",
    "\n",
    "Showing (II) $\\implies$ (I):\n",
    "\n",
    "Because it is the case that:\n",
    "\n",
    "$$\\lim_{n \\rightarrow \\infty} \\mathbb{E}[X_n] = b \\quad \\text{and} \\quad\\lim_{n \\rightarrow \\infty} \\text{Var}[X_n] = 0$$\n",
    "\n",
    "And also because $\\text{Var}[X_n] = \\mathbb{E}[X_n^2] - \\mathbb{E}[X_n]^2$, we have the following limit on the 2nd moment:\n",
    "\n",
    "$$\\lim_{n \\rightarrow \\infty} \\mathbb{E}[X_n^2] = b^2$$\n",
    "\n",
    "We now consider:\n",
    "\n",
    "$$\\lim_{n \\rightarrow \\infty} \\mathbb{E}[(X_n - b)^2] = \\lim_{n \\rightarrow \\infty} \\mathbb{E}[X_n^2] - 2b \\lim_{n \\rightarrow \\infty} \\mathbb{E}[X_n] + b^2 = b^2 - 2b(b) + b^2 = 0$$\n",
    "\n",
    "Which is the required result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Chapter 5, problem 5.\n",
    "\n",
    "Let $X_1,..., X_n \\sim \\text{Bernoulli}(p)$. Prove that:\n",
    "\n",
    "$$\\frac{1}{n} \\sum^n_{i=1} X^2_i \\overset{\\text{P}}{\\rightarrow} p \\quad \\text{and} \\quad \\frac{1}{n} \\sum^n_{i=1} X^2_i \\overset{\\text{qm}}{\\rightarrow} p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli random variables have mean $\\mathbb{E}[X_i] = p$ and variance $\\text{Var}[X_i] = p(1-p)$.\n",
    "\n",
    "We define a new random variable $Y_i = X^2_i$, and instead consider $Y_1, Y_2, ... , Y_n$. \n",
    "\n",
    "The mean of $Y_i$ is given by:\n",
    "\n",
    "$$\\mathbb{E}[Y_i] = \\mathbb{E}[X^2_i] = \\text{Var}[X_i] + \\mathbb{E}[X_i]^2 = p(1-p) + p^2 = p$$\n",
    "\n",
    "As each of the $X_i$ are Bernoulli random variables taking values of either 0 or 1, the random variables $Y_i = X^2_i$ are bounded within the interval $[0, 1]$.\n",
    "\n",
    "Applying Hoeffding's inequality to the sequence of sample means $\\bar{Y}_n$ and considering when $n \\rightarrow \\infty$ we have that $\\forall \\space \\epsilon > 0$:\n",
    "\n",
    "$$P(\\lvert \\bar{Y}_n - p \\rvert \\geq \\epsilon) \\leq 2e^{-2n \\epsilon^2} \\rightarrow 0$$\n",
    "\n",
    "As $Y_i = X^2_i$, we have that $\\bar{Y}_n = \\frac{1}{n} \\sum^n_{i=1} X^2_i$, and hence we have the required result concerning convergence in probability of the 2nd sample moment:\n",
    "\n",
    "$$\\bar{Y}_n \\overset{\\text{P}}{\\rightarrow} p \\implies \\frac{1}{n} \\sum^n_{i=1} X^2_i \\overset{\\text{P}}{\\rightarrow} p$$\n",
    "\n",
    "To show convergence in quadratic mean, we consider the expression $\\mathbb{E}[(\\bar{Y}_n - p)^2]$:\n",
    "\n",
    "$$\\mathbb{E}[(\\bar{Y}_n - p)^2] = \\mathbb{E}[(\\bar{Y}_n^2 - 2p \\bar{Y}_n + p^2)] = \\mathbb{E}[\\bar{Y}_n^2] - 2p \\mathbb{E}[Y_n] + p^2$$\n",
    "\n",
    "Substituting the variance and mean of the sample mean in place of its 2nd moment we have that:\n",
    "\n",
    "$$\\mathbb{E}[(\\bar{Y}_n - p)^2] = \\text{Var}[\\bar{Y}_n] + \\mathbb{E}[\\bar{Y}_n]^2 - 2p \\mathbb{E}[\\bar{Y}_n] + p^2$$\n",
    "\n",
    "As $\\bar{Y}_n$ is a sample mean, we have that $\\text{Var}[\\bar{Y}_n] = \\text{Var}[{Y_i}] / n$, and evaluating the expression for the variance of $Y_i$, we have that:\n",
    "\n",
    "$$\\text{Var}[Y_i] = \\mathbb{E}[Y^2_i] - \\mathbb{E}[Y_i]^2 = \\mathbb{E}[X^4_i] - p^2$$\n",
    "\n",
    "We now need to evaluate the fourth moment of the Bernoulli random variable $X_i$. We find the moment generating function $\\psi_{X_i}(t)$ as follows:\n",
    "\n",
    "$$\\psi_{X_i}(t) = \\mathbb{E}[e^{tX}] = \\int e^{tX} dF(x) = \\sum_{x_i} e^{t x_i} P(X_i = x_i) = e^{t} P(X_i = 1) + e^{0} P(X_i = 0)\\implies \\psi_{X_i}(t) = pe^t + (1-p) $$\n",
    "\n",
    "As $\\frac{d}{dt} e^t = e^t$ for all $t$, we have that the $k$th derivative of the moment generating function has the form:\n",
    "\n",
    "$$\\psi^{(k)}_{X_i}(t) = pe^t \\quad \\forall \\space k \\geq 1 $$\n",
    "\n",
    "Evaluating at $t= 0$, we find that for Bernoulli random variables, all $k$th moments have the form $\\psi^{(k)}_{X_i}(0) = \\mathbb{E}[X^{k}_i] = p$ for $k \\geq 1$.\n",
    "\n",
    "We then have that:\n",
    "\n",
    "$$\\text{Var}[\\bar{Y}_n] = \\frac{\\text{Var}[Y_i]}{n} = \\frac{p - p^2}{n} = \\frac{p(1-p)}{n}$$\n",
    "\n",
    "As $\\mathbb{E}[\\bar{Y}_n] = \\mathbb{E}[Y_i] = p$, we have that:\n",
    "\n",
    "$$\\mathbb{E}[(\\bar{Y}_n - p)^2] = \\text{Var}[\\bar{Y}_n] + \\mathbb{E}[\\bar{Y}_n]^2 - 2p \\mathbb{E}[\\bar{Y}_n] + p^2 = \\frac{p(1-p)}{n} + p^2 - 2p (p) + p^2 = \\frac{p(1-p)}{n}$$\n",
    "\n",
    "As $n \\rightarrow \\infty$, we have that $\\mathbb{E}[(\\bar{Y}_n - p)^2] = \\frac{p(1-p)}{n} \\rightarrow 0$, which is the required result.\n",
    "\n",
    "*It was only observed after completing the problem that for Bernoulli random variables, $X^2_i = X_i$, thereby rendering the need for computations using moment generating functions redundant in the calculation of $\\text{Var}[Y_i] = \\text{Var}[X^2_i]$, as it is the case that $\\mathbb{E}[X^2_i] = \\mathbb{E}[X_i] = p$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Chapter 5, problem 12.\n",
    "\n",
    "Let $X_1, X_2,...$ be random variables that are positive and integer valued.\n",
    "\n",
    "Show that $X_n \\overset{D}{\\rightarrow} X$ if and only if:\n",
    "\n",
    "$$\\lim_{n \\rightarrow \\infty} P(X_n = k) = P(X = k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Chapter 5, problem 15.\n",
    "\n",
    "\n",
    "Let\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "X_{11} \\\\\n",
    "X_{21} \\\\\n",
    "\\end{pmatrix},\n",
    "\\begin{pmatrix}\n",
    "X_{12} \\\\\n",
    "X_{22} \\\\\n",
    "\\end{pmatrix},\n",
    "...\n",
    "\\begin{pmatrix}\n",
    "X_{1n} \\\\\n",
    "X_{2n} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "be IID random vectors with mean $\\boldsymbol{\\mu} = (\\mu_1, \\mu_2)$ and variance $\\Sigma$. Assume that $\\mu_2 \\neq 0$. Then let\n",
    "\n",
    "$$\\bar{X}_1 = \\frac{1}{n} \\sum^n_{i=1} X_{1i}, \\quad \\bar{X}_2 = \\frac{1}{n} \\sum^n_{i=1} X_{2i}$$\n",
    "\n",
    "and define $Y_n = \\bar{X}_1 / \\bar{X}_2$. Find the limiting distribution of $Y_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the sample mean vector $\\bar{\\mathbf{X}}_n \\in \\mathbb{R}^2$  as follows:\n",
    "\n",
    "$$\\bar{\\mathbf{X}}_n =\n",
    "\\begin{pmatrix}\n",
    "\\bar{X}_1 \\\\\n",
    "\\bar{X}_2\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Then the multivariate CLT states that:\n",
    "\n",
    "$$\\sqrt{n} (\\bar{\\mathbf{X}}_n - \\boldsymbol{\\mu}) \\overset{\\text{d}}{\\rightarrow} N( \\boldsymbol{0}, \\Sigma)$$\n",
    "\n",
    "We now define the following scalar function of a vector, $g : \\mathbb{R}^2 \\mapsto \\mathbb{R}$:\n",
    "\n",
    "$$ \\quad g \\left[\\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} \\right] = \\frac{y_1}{y_2}$$ \n",
    "\n",
    "Which has gradient given by:\n",
    "\n",
    "$$\\nabla_{\\mathbf{y}} g( \\mathbf{y}) =\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial g}{\\partial y_1} \\\\\n",
    "\\frac{\\partial g}{\\partial y_2} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Denoting the gradient evaluated at the mean, $\\left. \\nabla_{\\mathbf{y}}g(\\mathbf{y}) \\right|_{\\mathbf{y} = \\boldsymbol{\\mu}}$ as $\\nabla_{\\boldsymbol{\\mu}}$, the multivariate Delta Method states that:\n",
    "\n",
    "$$\\sqrt{n}\\left( g(\\bar{\\mathbf{X}}_n) - g(\\boldsymbol{\\mu}) \\right) \\overset{\\text{d}}{\\rightarrow} N \\left( \\boldsymbol{0}, \\nabla^T_{\\boldsymbol{\\mu}} \\Sigma \\nabla_{\\boldsymbol{\\mu}} \\right) $$\n",
    "\n",
    "if $g(\\cdot)$ is smooth and differentiable.\n",
    "\n",
    "The partial derivatives of the scalar function $g(\\cdot)$ with respect to $y_1$ and $y_2$ are:\n",
    "\n",
    "$$\\frac{\\partial g}{\\partial x_1} =  \\frac{1}{x_2} \\quad, \\quad \\frac{\\partial g}{\\partial x_2} =  \\frac{-x_1}{x^2_2}$$\n",
    "\n",
    "Evaluating this at $\\mathbf{y} = \\boldsymbol{\\mu}$ we have that:\n",
    "\n",
    "$$\\nabla_{\\boldsymbol{\\mu}} = \n",
    "\\begin{pmatrix}\n",
    "\\frac{1}{\\mu_2} \\\\\n",
    "\\frac{-\\mu_1}{\\mu^2_2}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "In order to compute the asymptotic variance of $\\sqrt{n}\\left( g(\\bar{\\mathbf{X}}_n) - g(\\boldsymbol{\\mu}) \\right)$, we have to compute:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\nabla^T_{\\boldsymbol{\\mu}} \\Sigma \\nabla_{\\boldsymbol{\\mu}} &=\n",
    "\\begin{pmatrix}\n",
    "\\frac{1}{\\mu_2} , \\frac{-\\mu_1}{\\mu^2_2}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\sigma_{11} & \\sigma_{12} \\\\\n",
    "\\sigma_{21} & \\sigma_{22} \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\frac{1}{\\mu_2} \\\\\n",
    "\\frac{-\\mu_1}{\\mu^2_2} \\\\\n",
    "\\end{pmatrix} \\\\\n",
    "&= \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\sigma_{11}}{\\mu_2} - \\frac{\\sigma_{21} \\mu_1}{\\mu^2_2}, \\frac{\\sigma_{12}}{\\mu_2} - \\frac{\\sigma_{22} \\mu_1}{\\mu^2_2}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\frac{1}{\\mu_2} \\\\\n",
    "\\frac{-\\mu_1}{\\mu^2_2} \\\\\n",
    "\\end{pmatrix} \\\\\n",
    "&=\n",
    "\\frac{1}{\\mu_2} \\left(\\sigma_{11}{\\mu_2} - \\frac{\\sigma_{21} \\mu_1}{\\mu^2_2} \\right) - \\frac{\\mu_1}{\\mu^2_2} \\left( \\frac{\\sigma_{12}}{\\mu_2} - \\frac{\\sigma_{22} \\mu_1}{\\mu^2_2} \\right) \\\\\n",
    "&=\n",
    "\\left( \\frac{\\sigma_{11}}{\\mu^2_2} - \\frac{\\sigma_{21} \\mu_1}{\\mu^3_2} \\right) - \\left(  \\frac{\\mu_1 \\sigma_{12}}{\\mu^3_2} - \\frac{\\sigma_{22} \\mu_1^2}{\\mu^4_2} \\right) \\\\\n",
    "&= \\frac{\\sigma_{11} \\mu^2_2 - \\sigma_{21} \\mu_1 \\mu_2 - \\mu_1 \\mu_2 \\sigma_{12} + \\sigma_{22} \\mu^2_1}{\\mu^4_2} \\\\\n",
    "&= \\frac{1}{\\mu^4_2} \\left(\\sigma_{11} \\mu^2_2 - (\\sigma_{12} + \\sigma_{21}) \\mu_1 \\mu_2 + \\sigma_{22} \\mu^2_1 \\right)\n",
    "\\end{align}$$\n",
    "\n",
    "And we have the limiting distribution:\n",
    "\n",
    "$$\\sqrt{n} \\left( \\bar{X}_1 / \\bar{X}_2 - \\mu_1 / \\mu_2 \\right) \\overset{\\text{d}}{\\rightarrow} N \\left( 0, \\frac{1}{\\mu^4_2} (\\sigma_{11} \\mu^2_2 - (\\sigma_{12} + \\sigma_{21})\\mu_1 \\mu_2 + \\sigma_{22} \\mu^2_1 \\right)$$\n",
    "\n",
    "Which yields the following result on the limiting distribution of $Y_n$:\n",
    "\n",
    "$$Y_n = \\frac{\\bar{X}_1}{\\bar{X}_2} \\overset{\\text{d}}{\\rightarrow} N \\left( \\frac{\\mu_1}{\\mu_2}, \\frac{\\sigma^2}{n}  \\right)$$\n",
    "\n",
    "where $\\sigma^2 = \\frac{1}{\\mu^4_2} (\\sigma_{11} \\mu^2_2 - (\\sigma_{12} + \\sigma_{21})\\mu_1 \\mu_2 + \\sigma_{22} \\mu^2_1$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
