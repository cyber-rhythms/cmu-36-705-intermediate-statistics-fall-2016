{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 36-705 Intermediate Statistics.\n",
    "\n",
    "### Homework 5.\n",
    "\n",
    "### INSERT DATE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Chapter 9, problem 1.\n",
    "\n",
    "Let $X_1, X_2,..., X_n \\sim \\text{Gamma}(a,b)$, where the $X_i$ are IID. Find the method of moments estimator for $a$ and $b$.\n",
    "\n",
    "A random variable $X_i$ with gamma distribution has the following mean and variance:\n",
    "\n",
    "$$\\mathbb{E}[X_i] = ab$$\n",
    "\n",
    "$$\\mathbb{Var}[X_i] = ab^2$$\n",
    "\n",
    "Hence the second moment of the distribution is given by:\n",
    "\n",
    "$$\\mathbb{E}[X^2] = \\text{Var}[X_i] + \\mathbb{E}[X_i]^2 = ab^2 + a^2b^2 = ab^2(1+a) = ab(b+ab)$$\n",
    "\n",
    "To find the method of moments estimators of the parameters $a$ and $b$, that is $\\widehat{a}_{\\text{MoM}}$ and $\\widehat{b}_{\\text{MoM}}$, we equate the 1st and 2nd sample moments with the theoretical moments to get 2 equations.\n",
    "\n",
    "Denoting the kth sample moment as $m_k = \\frac{1}{n} \\sum^n_{i=1} X^k_i$, we have that:\n",
    "\n",
    "$$m_1 = \\mathbb{E}[X_i] \\implies \\frac{1}{n} \\sum^n_{i=1} X_i = ab$$\n",
    "\n",
    "$$m_2 = \\mathbb{E}[X^2_i] \\implies \\frac{1}{n} \\sum^n_{i=1} X^2_i = ab^2(1+a)$$\n",
    "\n",
    "Rearranging the 1st equation yields $b = \\frac{1}{a} \\bar{X}_n$, which we substitute into the 2nd equation to yield:\n",
    "\n",
    "$$\\begin{align}\n",
    "M_2 &= \\frac{1}{n} \\sum^n_{i=1} X^2_i = \\bar{X}_n \\left( \\frac{1}{a} \\bar{X}_n + \\bar{X}_n \\right) \\\\\n",
    "\\implies \\frac{1}{n} \\sum^n_{i=1} X^2_i &= \\frac{1}{a} \\bar{X}^2_n + \\bar{X}^2_n \\\\\n",
    "\\implies \\frac{1}{a}\\bar{X}^2_n &= \\frac{1}{n} \\sum^n_{i=1} X^2_i - \\bar{X}^2_n\n",
    "\\end{align}$$\n",
    "\n",
    "Hence we have that:\n",
    "\n",
    "$$\\widehat{a}_{\\text{MoM}} = \\frac{\\bar{X}^2_n}{\\frac{1}{n} \\sum^n_{i=1} X^2_i - \\bar{X}^2_n} = \\frac{m^2_1}{m_2 - m^2_1}$$\n",
    "\n",
    "$$\\widehat{b} = \\frac{\\frac{1}{n} \\sum^n_{i=1} X^2_i - \\bar{X}^2_n}{\\bar{X}_n} = \\frac{m_2 - m_1^2}{m1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Chapter 9, problem 5.\n",
    "\n",
    "Let the $X_1,..., X_n \\sim Po(\\lambda)$, i.e. have a Poisson distribution parametrised by $\\lambda$.\n",
    "\n",
    "Find the:\n",
    "\n",
    "a) Method of moments estimator.\n",
    "\n",
    "b) Maximum likelihood estimator.\n",
    "\n",
    "c) Fisher information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a)\n",
    "\n",
    "Equating the 1st sample moment with the 1st theoretical moment, i.e. the mean, we have:\n",
    "\n",
    "$$\\frac{1}{n} \\sum^n_{i=1} X_i = \\mathbb{E}[X_i] = \\lambda$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\\widehat{\\lambda}_{\\text{MoM}} = \\bar{X}_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b) \n",
    "\n",
    "The Poisson distribution has probability mass function:\n",
    "\n",
    "$$f_{X_i}(x_i) = e^{- \\lambda}\\frac{\\lambda^{x_i}}{x_i!} \\quad x_i \\geq 0$$\n",
    "\n",
    "The likelihood function $L(\\lambda)$ is a function of the parameter $\\lambda$ and is given by the product of individual Poission probability mass functions, as the data is IID.\n",
    "\n",
    "The value of the parameter that maximises the log-likelihood function $l(\\lambda)$ evaluated at the values of the data $(x_1, x_2, ..., x_n)$ is the maximum likelihood estimator, $\\widehat{\\lambda}_{\\text{ML}}$:\n",
    "\n",
    "$$\\widehat{\\lambda}_{\\text{ML}} = \\underset{\\lambda}{\\text{argmax}} \\prod^n_{i=1} \\log f_{X_i}(x_i)$$\n",
    "\n",
    "The likelihood function $L(\\lambda)$ is:\n",
    "\n",
    "$$L(\\lambda) = \\prod^n_{i=1} f_{X_i}(x_i) = \\prod^n_{i=1} e^{- \\lambda}\\frac{\\lambda^{x_i}}{x_i !} = \\frac{e^{-n \\lambda} \\lambda^{\\sum^n_{i=1} x_i}} {\\prod^n_{i=1} (x_i!)}$$\n",
    "\n",
    "The log-likelihood function $l(\\lambda)$ is:\n",
    "\n",
    "$$l(\\lambda) = \\log \\frac{e^{-n \\lambda} \\lambda^{\\sum^n_{i=1} x_i}} {\\prod^n_{i=1} (x_i!)} = -n \\lambda + \\sum^n_{i=1} \\log \\lambda - \\sum^n_{i=1} \\log (x_i !)$$\n",
    "\n",
    "Taking derivatives with respect to $\\lambda$ and setting to 0, we have that:\n",
    "\n",
    "$$\\frac{d}{d \\lambda} = -n + \\sum^n_{i=1} x_i \\left(\\frac{1}{\\lambda} \\right) = 0$$\n",
    "\n",
    "Hence we have that $\\lambda_{\\text{ML}} = \\bar{X}_n$, i.e. the maxmimum likelihood estimator of the Poisson parameter is the sample mean, which coincides with that of the method of moments estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2c)\n",
    "\n",
    "For $n$ observations, the Fisher information, $I_n(\\lambda)$, is the variance of the score function, and the latter is the partial derivative of the log-likelihood. Suppressing the dependence of the score function on the data, we have that:\n",
    "\n",
    "$$I_n(\\lambda) = \\text{Var}_{\\lambda}(s_n (\\lambda)) = \\text{Var}_{\\lambda}[l'(\\lambda)]$$\n",
    "\n",
    "$$s_n (\\lambda) = l'(\\lambda) = -n + \\frac{1}{\\lambda} \\sum^n_{i=1} x_i$$\n",
    "\n",
    "$$\\text{Var}_{\\lambda}[s_n (\\lambda)] = \\text{Var} \\left[ -n + \\frac{1}{\\lambda} \\sum^n_{i=1} x_i  \\right] = \\frac{1}{\\lambda^2} \\left( \\sum^n_{i=1} x_i \\right)$$\n",
    "\n",
    "As the data $X_i$ are IID, we have that $\\text{Cov}[X_i, X_j] = 0$, meaning that:\n",
    "\n",
    "$$\\text{Var}_{\\lambda}[s_n (\\lambda)] = \\text{Var}\\left[-n + \\frac{1}{\\lambda} \\sum^n_{i=1} \\right] \\frac{1}{\\lambda^2} \\sum^n_{i=1} \\text{Var}[X_i] = \\frac{1}{\\lambda^2} n \\lambda = \\frac{n}{\\lambda}$$\n",
    "\n",
    "Hence the Fisher information $I_n(\\lambda) = \\frac{n}{\\lambda}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Chapter 9, problem 7 (a, c, d, e)\n",
    "\n",
    "Let $X_1, ... ,X_n \\sim N(\\theta ,1)$, where the $X_i$ are IID.\n",
    "\n",
    "Define:\n",
    "\n",
    "$$Y_i = \\begin{cases}\n",
    "1\\quad \\text{if} \\quad X_i > 0 \\\\\n",
    "0\\quad \\text{if} \\quad X_i \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "Let $\\psi = P(Y_i = 1)$.\n",
    "\n",
    "a) Find the maximum likelihood estimator $\\widehat{\\psi}$ of $\\psi$.\n",
    "\n",
    "b) Define $\\tilde{\\psi} = \\frac{1}{n} \\sum^n_{i=1} Y_i$, and show that $\\tilde{\\psi}$ is a consistent estimator of $\\psi$.\n",
    "\n",
    "d) Compute the asymptotic relative efficiency of $\\tilde{\\psi}$ to $\\widehat{\\psi}$ and use the Delta method to get the standard error of maximum likelihood estimator. Then compute the standard error of $\\tilde{\\psi}$.\n",
    "\n",
    "e) Suppose that the data is not Normal. Show that the $\\widehat{\\psi}$ is not consistent. What, if anything, does $\\widehat{\\psi}$ converge to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3a)\n",
    "\n",
    "The random variables $Y_i$ have Bernoulli distribution with parameter $\\psi$.\n",
    "\n",
    "We have that:\n",
    "\n",
    "$$\\psi = P(Y_i = 1) = P(X_i > 0) = 1 - P(X_i < 0) = 1 - P \\left( z < \\frac{-\\theta}{1} \\right) = 1 - \\Phi(-\\theta) = \\Phi(\\theta)$$\n",
    "\n",
    "Therefore we also have that $1 - \\psi = \\Phi(-\\theta) = 1 - \\Phi(\\theta)$\n",
    "\n",
    "As the $Y_i$ have a Bernoulli distribution, the mean and variance of the $Y_i$ are given by $\\mathbb{E}[Y_i] = \\psi$ and $\\text{Var}[Y_i] = \\psi (1 - \\psi)$.\n",
    "\n",
    "The maximum likelihood estimator $\\widehat{\\theta}$ of the parameter $\\theta$ is the sample mean, $\\widehat{\\theta} = \\bar{X}_n = \\sum^n_{i=1} X_i$.\n",
    "\n",
    "Noting that our parameter $\\psi = g(\\theta) = \\Phi(\\theta)$, we can use the equivariance property of the maximum likelihood estimator to yield:\n",
    "\n",
    "$$\\widehat{\\psi} = g(\\widehat{\\theta}) = \\Phi(\\widehat{\\theta}) = \\Phi(\\bar{X}_n)$$\n",
    "\n",
    "* Is this result contingent on regularity conditions, or is tbat more specifically for asymptotic results on MLE such as consistency and asymptotic Normality. Semms like consistency is a statistical concern, whereas the setting above is probabilistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3b)\n",
    "\n",
    "The proposed estimator $\\tilde{\\psi}$ is the sample mean of the Bernoulli random variables $Y_i$, and the parameter $\\psi$ is the population mean of the Bernoulli distribution, $\\psi = \\mathbb{E}[Y_i]$.\n",
    "\n",
    "As the $X_i$ are IID, and tbe Bernoulli random variables $Y_i$ have been defined as indicator functions dependent on the positivity of the $X_i$, the former are also IID.\n",
    "\n",
    "The weak law of large numbers (WLLN) for IID random variables, the sequence of sample means converges in probability to the population mean.  We have that for all $\\epsilon > 0$, as $n \\rightarrow \\infty$:\n",
    "\n",
    "$$P(|\\bar{Y}_n - \\mathbb{E}[Y_i]| > \\epsilon) \\rightarrow 0 \\implies P(|\\tilde{\\psi} - \\psi| > \\epsilon) \\rightarrow 0 \\implies \\tilde{\\psi} \\overset{P}{\\rightarrow} \\psi$$\n",
    "\n",
    "Hence $\\tilde{\\psi}$ is a consistent estimator the parameter $\\psi$.\n",
    "\n",
    "Alternatively, as the proposed estimator $\\tilde{\\psi}$ is a sample mean, it is an unbiased estimator of the parameter $\\psi$, i.e. $B = 0$. Hence the mean squared error of the estimator will equal to the variance of the estimator:\n",
    "\n",
    "$$\\text{MSE} = \\mathbb{E}[(\\tilde{\\psi} - \\psi)^2] = B^2 + V = V = \\text{Var}_{\\psi}[\\tilde{\\psi}] = \\frac{\\psi (1 - \\psi)}{n}$$\n",
    "\n",
    "As $n \\rightarrow \\infty$, we have that $\\text{MSE} \\rightarrow 0$, and we have convergence in quadratic mean of the estimator to the parameter, $\\tilde{\\psi} \\overset{\\text{q.m.}}{\\rightarrow} \\psi$, which implies convergence in probability, $\\tilde{\\psi} \\overset{P}\\rightarrow \\psi$.\n",
    "\n",
    "* Do we have regularity conditions to think about here?\n",
    "* Consistency of the MLE, vs WLLn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3c)\n",
    "\n",
    "The asymptotic relative efficiency (ARE) of $\\widetilde{\\psi}$ to $\\widehat{\\psi}$, is the following ratio of their asymptotic variances:\n",
    "\n",
    "$$\\text{ARE}(\\widetilde{\\psi}, \\widehat{\\psi}) = \\frac{\\sigma^2_{\\widehat{\\psi}}}{\\sigma^2_{\\widetilde{\\psi}}}$$\n",
    "\n",
    "This comparison valid only if the two estimators are asymptotically Normal. We begin by computing the asymptotic variance of $\\widehat{\\psi}$ followed by that of $\\widetilde{\\psi}$.\n",
    "\n",
    "Recall that $\\widehat{\\psi} = g(\\widehat{\\theta})$, i.e. a function of a maximum likelihood estimator. Asymptotic Normality can be established by the delta method, which under appropriate conditions, states that:\n",
    "\n",
    "$$\\sqrt{n}(g(\\widehat{\\theta}) - g(\\theta)) \\overset{d}{\\rightarrow} N\\left(0, \\frac{g'(\\theta)^2}{I(\\theta)}\\right) \\implies (g(\\widehat{\\theta}) - g(\\theta)) \\overset{d}{\\rightarrow} N\\left(0, \\frac{g'(\\theta)^2}{I_n(\\theta)}\\right) \\implies g(\\widehat{\\theta}) \\overset{d}{\\rightarrow} N \\left(g(\\theta), \\frac{g'(\\theta)^2}{I_n(\\theta)} \\right)$$\n",
    "\n",
    "Where $I_n(\\theta)$ is the Fisher information on the entire dataset and $I(\\theta)$ is the Fisher information of a single datapoint. As the data are IID, we have that $I_n(\\theta) = nI(\\theta)$.\n",
    "\n",
    "Hence we have that the estimator $\\widehat{\\psi}$ is asymptotically Normal with mean $g(\\theta)$ and (asymptotic) variance:\n",
    "\n",
    "$$\\sigma^2_{\\widehat{\\psi}} = \\frac{g'(\\theta)^2}{I_n(\\theta)}$$\n",
    "\n",
    "As $g(\\theta) = \\Phi(\\theta)$, which is the standard Normal cumulative distribution function, we have that $g'(\\theta)^2 = \\phi^2(\\theta)$, where $\\phi(\\cdot)$ is the standard Normal probability density function.\n",
    "\n",
    "To compute the Fisher information over the entire dataset, $I_n(\\theta)$, we use the result that it is equal to the variance of the score function over the entire dataset, $s_n(\\theta, x_1,..., x_n)$:\n",
    "\n",
    "$$I_n(\\theta) = \\text{Var}_{\\theta}[s_n(\\theta)]$$\n",
    "\n",
    "Where we have suppressed the dependence of the score on the data for notational brevity.\n",
    "\n",
    "As the score function is equal to the derivative of the log-likelihood, $l'(\\theta)$, denoting the PDF of the datapoint $X_i$ as $f_{X_i}$ we have that:\n",
    "\n",
    "$$\\begin{align}\n",
    "s_n(\\theta) = l'(\\theta) &= \\frac{\\partial}{\\partial \\theta} \\log \\prod^n_{i=1} f_{X_i}(x_i ; \\theta) \\\\\n",
    "&= \\frac{\\partial}{\\partial \\theta} \\log \\prod^n_{i=1} \\frac{1}{\\sqrt{2 \\pi}} \\exp \\left( -\\frac{1}{2} (x_i - \\theta)^2 \\right) \\\\\n",
    "&= \\frac{\\partial}{\\partial \\theta} \\sum^n_{i=1} -\\frac{1}{2} \\log(2 \\pi) - \\frac{1}{2} (x_i - \\theta)^2 \\\\\n",
    "&= -\\frac{1}{2} \\sum^n_{i=1} \\frac{\\partial}{\\partial \\theta} \\left( \\log(2 \\pi) + (x_i - \\theta)^2 \\right) \\\\\n",
    "&= -\\frac{1}{2} \\sum^n_{i=1} -2x_i + 2\\theta \\\\\n",
    "&= \\left(\\sum^n_{i=1} x_i \\right) - n\\theta \\\\\n",
    "\\implies s_n(\\theta) &=  n(\\bar{X}_n - \\theta)\n",
    "\\end{align}$$\n",
    "\n",
    "As the mean of the score function is 0, that is $\\mathbb{E}_{\\theta}[s_n(\\theta)] = 0$, the variance of the score function is equal to its second moment:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{Var}_{\\theta}[s_n(\\theta)] = \\mathbb{E}_{\\theta}[s^2_n(\\theta)] &= \\mathbb{E}_{\\theta}[n^2(\\bar{X}_n - \\theta)^2] \\\\\n",
    "&= n^2 \\left( \\mathbb{E}[\\bar{X}_n^2] - 2 \\theta \\mathbb{E}[\\bar{X}_n] + \\theta^2 \\right) \\\\\n",
    "&= n^2 \\left[\\left( \\frac{1}{n} + \\theta^2 \\right) - 2 \\theta^2 + \\theta^2   \\right] \\\\\n",
    "&= n^2 \\left( \\frac{1}{n} \\right) \\\\\n",
    "\\implies \\text{Var}_{\\theta}[s_n(\\theta)] &= n \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Where we have used the fact that $\\bar{X}_n$ is a sample mean with mean $\\mathbb{E}[\\bar{X}_n] = \\theta$, variance $\\text{Var}[\\bar{X_n}] = \\frac{1}{n}$ and 2nd moment $\\mathbb{E}[\\bar{X}_n^2] = \\text{Var}[\\bar{X}_n] + \\mathbb{E}[\\bar{X}_n]^2 = \\frac{1}{n} + \\theta$.\n",
    "\n",
    "Hence we have the Fisher information over the entire dataset $I_n(\\theta) = n$, and together with $g'(\\theta) = \\phi^2(\\theta)$, this yields the following asymptotic variance on $\\widehat{\\psi}$: \n",
    "\n",
    "$$\\sigma^2_{\\widehat{\\psi}} = \\frac{\\phi^2(\\theta)}{n}$$\n",
    "\n",
    "And therefore the estimator $\\widehat{\\psi}$ has the following asymptotically Normal distribution:\n",
    "\n",
    "$$\\widehat{\\psi} \\approx \\left(\\Phi(\\theta), \\frac{\\phi^2(\\theta)}{n} \\right)$$\n",
    "\n",
    "We now compute the asymptotic variance of the estimator $\\widetilde{\\psi} = \\sum^n_{i=1} Y_i$. As this estimator is the maximum likelihood estimator of the Bernoulli parameter $\\psi$, and also the sample mean of the data $Y_i$, we can use the asymptotic Normality of the maximum likelihood estimator or the Central Limit Theorem on the sample mean to yield:\n",
    "\n",
    "$$\\frac{\\sqrt{n}(\\bar{Y}_n - \\mathbb{E}[Y_i])}{\\text{Var}[Y_i]} \\overset{d}{\\rightarrow} N(0,1) \\implies \\widetilde{\\psi} \\overset{d}{\\rightarrow} N \\left(\\psi, \\frac{\\psi (1 - \\psi)}{n} \\right)$$\n",
    "\n",
    "Hence the estimator $\\widetilde{\\psi}$ has the following asymptotically Normal distribution:\n",
    "\n",
    "$$\\widetilde{\\psi} \\approx N \\left(\\Phi(\\theta), \\frac{\\Phi(\\theta)(1 - \\Phi(\\theta))}{n} \\right)$$\n",
    "\n",
    "And its asymptotic variance is given by:\n",
    "\n",
    "$$\\sigma^2_{\\widetilde{\\psi}} = \\frac{\\Phi(\\theta)(1 - \\Phi(\\theta))}{n}$$\n",
    "\n",
    "And hence we have that the asymptotic relative efficiency of the two estimators is:\n",
    "\n",
    "$$\\text{ARE}(\\widetilde{\\psi}, \\widehat{\\psi}) = \\frac{\\sigma^2_{\\widehat{\\psi}}}{\\sigma^2_{\\widetilde{\\psi}}} = \\frac{\\phi^2(\\theta)}{\\Phi(\\theta)(1 - \\Phi(\\theta))}$$\n",
    "\n",
    "The asymptotic standard errors of the two estimators are given by:\n",
    "\n",
    "$$\\sigma_{\\widehat{\\psi}} = \\frac{\\phi(\\theta)}{\\sqrt{n}} \\quad, \\quad \\sigma_{\\widetilde{\\psi}} = \\sqrt{\\frac{\\Phi(\\theta)(1 - \\Phi(\\theta)}{n}}$$\n",
    "\n",
    "Plugging in the maximum likelihood estimate, $\\widehat{\\theta}$ of the parameter $\\theta$ yields the following estimated standard errors:\n",
    "\n",
    "$$\\widehat{\\text{se}}(\\widehat{\\psi}) = \\frac{\\phi(\\widehat{\\theta})}{\\sqrt{n}} =  \\frac{\\phi(\\bar{X}_n)}{\\sqrt{n}} \\quad, \\quad \\widehat{\\text{se}}(\\widetilde{\\psi}) = \\sqrt{\\frac{\\Phi(\\widehat{\\theta})(1 - \\Phi(\\widehat{\\theta})}{n}} = \\sqrt{\\frac{\\Phi(\\bar{X}_n)(1 - \\Phi(\\bar{X}_n)}{n}}$$\n",
    "\n",
    "These delta method also allows for the use of these estimated standard errors to yield the following asymptotic distributions:\n",
    "\n",
    "$$\\widehat{\\psi} \\overset{d}{\\rightarrow} N \\left( \\Phi(\\theta), \\frac{\\phi^2(\\bar{X}_n)}{n} \\right)$$\n",
    "\n",
    "And\n",
    "\n",
    "$$\\widetilde{\\psi} \\overset{d}{\\rightarrow} N \\left(\\Phi(\\theta), \\frac{\\Phi(\\bar{X}_n)(1 - \\Phi(\\bar{X}_n)}{n} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Chapter 9, problem 7 (a, b, c).\n",
    "\n",
    "Comparing two treatments, we have that $n_1$ people are given treatment 1 and $n_2$ people are given treatment 2. Let $X_1$ be the number of people treatment 1 who respond favourably to the treatment, and let $X_2$ be the number of people on treatment 2 who respond favourably.\n",
    "\n",
    "Assume that $X_1 \\sim \\text{Binomial}(n_1, p_1)$, and $X_2 \\sim \\text{Binomial}(n_2, p_2)$; and let $\\psi = p_1 - p_2$.\n",
    "\n",
    "a) Find the maximum likelihood estimator $\\widehat{\\psi}$ for $\\psi$.\n",
    "\n",
    "b) Find the Fisher information matrix $I(p_1, p_2)$.\n",
    "\n",
    "c) Use the multiparameter delta method to find the asymptotic standard error of $\\widehat{\\psi}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4a)\n",
    "\n",
    "In this case, we assume that the the number of people who respond favourably to treatment 1 and treatment 2 are independent. That is, $X_1 \\bot X_2$. The setting we have differs from the usual IID setting because by modelling the number of people who respond to treatment 1 and treatment 2 with distinct Binomial distributions with their own sets of parameters means that the two random variables are not identically distributed - we are dealing with a multiparameter problem.\n",
    "\n",
    "A discrete random variable $X_i$ with Binomial distribution, $X_i \\sim \\text{Binomial} (n_i, p_i)$ has the following probability mass function:\n",
    "\n",
    "$$f_{X_i}(x_i ; n_i, p_i) = P(X_i = x_i) = \\binom{n_i}{x_i} p_i^{x_i} (1 - p_1)^{n_i - x_i}$$\n",
    "\n",
    "We stack the scalar parameters and maximum likelihood estimators as vectors like so:\n",
    "\n",
    "$$\\mathbf{p} =\n",
    "\\begin{pmatrix}\n",
    "p_1 \\\\\n",
    "p_2\n",
    "\\end{pmatrix} \\quad\n",
    "\\widehat{\\mathbf{p}} = \n",
    "\\begin{pmatrix}\n",
    "\\widehat{p}_1 \\\\\n",
    "\\widehat{p}_2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We treat the number of trials parameters, $\\mathbf{n} = (n_1, n_2)^T$ in the Binomial distributions as nuisance parameters, and denote their maximum likelihood estimators as $\\widehat{\\mathbf{n}}$. \n",
    "\n",
    "The likelihood function $L(\\mathbf{n}, \\mathbf{p})$ is a function of all parameters, including our nuisance parameters. We now define the profile likelihood for our parameters of interest, $\\mathbf{p}$, which is the likelihood function maximised only with respect to our nuisance parameters:\n",
    "\n",
    "$$L(\\mathbf{p}) = \\sup_{\\mathbf{n}} L(\\mathbf{n}, \\mathbf{p}) = L(\\widehat{\\mathbf{n}}, \\mathbf{p})$$\n",
    "\n",
    "We now consider maximising the profile log-likelihood with respect to the parameter of interest $\\mathbf{p}$:\n",
    "\n",
    "$$\\widehat{\\mathbf{p}} = \\underset{\\mathbf{p}}{\\text{argmax}} \\space l(\\mathbf{p}) = \\sum^2_{i=1} \\log \\binom{\\widehat{n}_i}{x_i} + x_i \\log p_i + (\\widehat{n}_i - x_i) \\log (1 - p_i)$$\n",
    "\n",
    "As the parameters are not coupled, we can treat this maximisation as 2 independent maximisation problems. Taking the derivative with respect to $p_1$ and setting to 0, we have that:\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial p_1} = \\frac{x_1}{p_1} - \\frac{\\widehat{n}_1 - x_1}{p_1} = \\frac{x_1 - p_1 x_1 - \\widehat{n}_1 p_1 + p_1 x_1}{p_1(1 - p_1)} \\implies p_1 = \\frac{x_1}{\\widehat{n}_1}$$\n",
    "\n",
    "And applying similar reasoning to $p_2$, we have the maximum likelihood estimator:\n",
    "\n",
    "$$\\widehat{\\mathbf{p}} = \n",
    "\\begin{pmatrix}\n",
    "\\frac{x_1}{\\widehat{n}_1} \\\\\n",
    "\\frac{x_2}{\\widehat{n}_2} \\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "We now define the following scalar function of the parameter vector $\\psi = g(\\mathbf{p}) = p_1 - p_2$ where $g: \\mathbb{R}^2 \\mapsto \\mathbb{R}$.  Using equivariance of the maximum likelihood estimator we have that:\n",
    "\n",
    "$$\\widehat{\\psi} = g(\\widehat{\\mathbf{p}}) = \\widehat{p}_1 - \\widehat{p}_2 = \\frac{x_1}{\\widehat{n}_1} - \\frac{x_2}{\\widehat{n}_2}$$\n",
    "\n",
    "Which is the maximum likelihood estimator for $\\psi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4b)\n",
    "\n",
    "The Fisher information matrix $I(p_1, p_2) \\in \\mathbb{R}^2$ can be computing by taking negative expectations of the Hessian matrix, $\\mathbf{H}$. Where the Hessian matrix is with respect to the scalar-valued log-likelihood function, and with expectations being taken over the joint distribution of the data at the true value of the parameter:\n",
    "\n",
    "$$I(p_1, p_2)_{ij} = -\\mathbb{E}_{\\mathbf{p}}[H_{ij}] = -\\mathbb{E}_{\\mathbf{p}}\\left[\\frac{\\partial^2 l(\\mathbf{p})}{\\partial p_i \\partial p_j} \\right]$$\n",
    "\n",
    "The Hessian of the scalar-valued log-likelihood function is equivalent to the Jacobian matrix, $\\mathbf{J}$ of the gradient of the log-likelihood $\\nabla_{\\mathbf{p}} l(\\mathbf{p})$. And the gradient of the scalar-valued log-likelihood function is the vector-valued score function $s(\\mathbf{n}, \\mathbf{p})$:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{p}}l(\\mathbf{p}) = s(\\mathbf{n}, \\mathbf{p}) =\n",
    "\\begin{pmatrix}\n",
    "\\frac{1}{p_1(1 - p_1)} x_1 - n_1 p_1 \\\\\n",
    "\\frac{1}{p_2(1 - p_2)} x_2 - n_2 p_2 \\\\\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "s_1 \\\\\n",
    "s_2 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We have the following derivatives of the score function:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial s_1}{\\partial p_1} &= \\frac{\\partial}{\\partial p_1} \\left(\\frac{x_1 - n_1 p_1}{p_1(1-p_1)} \\right) \\\\\n",
    "&= \\frac{-n_1[p_1(1 - p_1)] - (x_1 - n_1 p_1) (1 - 2p_1)}{[p_1 (1 - p_1)]^2} \\\\\n",
    "&= \\frac{-n_1 p_1 + n_1 p_1^2 - (x_1 - 2p_1 x_1 - n_1 p_1 + 2n_1 p_1^2)}{[p_1(1 - p_1)]^2} \\\\\n",
    "\\implies \\frac{\\partial s_1}{\\partial p_1} &= \\frac{-n_1 p_1^2 - x_1 + 2 p_1 x_1}{[p_1 (1 - p_1)]^2} \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "And the cross-derivatives of the log-likelihood are 0:\n",
    "\n",
    "$$\\frac{\\partial s_1}{\\partial p_2} = \\frac{\\partial}{\\partial p_2} \\left(\\frac{x_1 - n_1 p_1}{p_1 ( 1- p_1)} \\right) = 0$$\n",
    "\n",
    "Hence we have the following Hessian:\n",
    "\n",
    "$$\\mathbf{H} =\n",
    "\\begin{pmatrix}\n",
    "\\frac{-n_1 p_1^2 - x_1 + 2p_1 x_1}{[p_1(1-p_1)]^2} & 0 \\\\\n",
    "0 & \\frac{-n_2 p_2^2 - x_2 + 2p_2 x_2}{[p_2(1-p_2)]^2}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "Computing negative expectations on the non-zero diagonal entries, we have:\n",
    "\n",
    "$$\\begin{align}\n",
    "- \\mathbb{E}[H_{11}] &= -\\frac{1}{[p_1 ( 1- p_1)]^2} \\mathbb{E}[-n_1 p^2_1 - x_1 + 2p_1 x_1] \\\\\n",
    "&= -\\frac{1}{[p_1 ( 1- p_1)]^2} (-n_1 p^2_1 - \\mathbb{E}[x_1] + 2p_1 \\mathbb{E}[x_1]) \\\\\n",
    "&= -\\frac{1}{[p_1 ( 1- p_1)]^2} (-n_1 p^2_1 - n_1 p_1 + 2 n_1 p_1^2) \\\\\n",
    "&= \\frac{n_1 p_1 - n_1 p_1^2}{[p_1 ( 1- p_1)]^2} \\\\\n",
    "&= \\frac{n_1}{p_1 (1 - p_1)}\n",
    "\\end{align}$$\n",
    "\n",
    "Yielding the following Fisher information matrix:\n",
    "\n",
    "$$\n",
    "I(p_1, p_2) =\n",
    "\\begin{pmatrix}\n",
    "\\frac{n_1}{p_1 (1 - p_1)} & 0 \\\\\n",
    "0 & \\frac{n_2}{p_2 (1 - p_2)} \\\\\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4c)\n",
    "\n",
    "In order to compute the asymptotic standard error, and then the estimated standard errors, we use the multiparameter delta method, which states that for $\\psi = g(\\mathbf{p})$ and $\\widehat{\\psi} = g(\\mathbf{\\widehat{p}})$:\n",
    "\n",
    "$$\\frac{\\widehat{\\psi} - \\psi}{\\text{se}({\\psi})} \\overset{d}{\\rightarrow} N(0,1) \\implies \\widehat{\\psi} \\overset{d}{\\rightarrow} N(\\psi, \\text{se}^2(\\psi))$$\n",
    "\n",
    "Denoting the gradient of $g(\\cdot)$ evaluated at the value of the parameter as $\\nabla_{\\mathbf{p}}g(\\mathbf{p})$; the asymptotic standard error, $\\text{se}(\\psi)$ is given by:\n",
    "\n",
    "$$\\text{se}(\\psi) = \\sqrt{\\nabla_{\\mathbf{p}}^T I^{-1}(p_1, p_2) \\nabla_{\\mathbf{p}}}$$\n",
    "\n",
    "We have that the gradient of $g(\\cdot)$ is given by:\n",
    "\n",
    "$$\\nabla_{\\mathbf{p}} g(\\mathbf{p}) =\n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial g}{\\partial p_1} \\\\\n",
    "\\frac{\\partial g}{\\partial p_2} \\\\\n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial}{\\partial p_1} (p_1 - p_2) \\\\\n",
    "\\frac{\\partial}{\\partial p_2} (p_1 - p_2) \\\\\n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "-1 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "And as the Fisher information matrix is a diagonal matrix, we have that:\n",
    "\n",
    "$$I^{-1}(p_1, p_2) =\n",
    "\\begin{pmatrix}\n",
    "I_{11}^{-1} & 0 \\\\\n",
    "0 & I_{22}^{-1} \\\\\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "\\frac{p_1 (1 - p_1)}{n_1} & 0 \\\\\n",
    "0 & \\frac{p_2(1 - p_2)}{n_2} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Yielding the following asymptotic standard errors:\n",
    "\n",
    "$$\\text{se}(\\psi) = \\sqrt{\\frac{p_1(1 - p_1)}{n_1} + \\frac{p_2(1 - p_2)}{n_2}}$$\n",
    "\n",
    "Plugging in the maximum likelihood estimators we derived, we have the following estimated asymptotic standard errors:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\widehat{\\text{se}}(\\widehat{\\psi}) &= \\sqrt{\\frac{\\widehat{p}_1(1 - \\widehat{p}_1)}{n_1} + \\frac{\\widehat{p}_2(1 - \\widehat{p}_2)}{n_2}}\n",
    "\\end{align}$$\n",
    "\n",
    "And the above asymptotic distribution on $\\widehat{\\psi}$ also holds if we replace the asymptotic standard errors $\\text{se}(\\psi)$ with their estimates $\\widehat{\\text{se}}(\\widehat{\\psi})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)\n",
    "\n",
    "Let $\\theta$ be a scalar parameter.\n",
    "\n",
    "a) Let $\\psi = g(\\theta)$ where $g$ is a smooth, invertible function. Hence, $\\theta = h(\\psi)$, where $h = g^{-1}$. Let $I(\\theta)$ denote the Fisher information for $\\theta$ and let $I(\\psi)$ denote the Fisher information for $\\psi$. Show that $I(\\psi) = I(\\theta) (h'(\\psi))^2$.\n",
    "\n",
    "b) Let $X \\sim N(\\theta, 1)$ and let $\\psi = e^{\\theta}$. Find the Fisher information for $\\psi$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
